{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# addresses of the files\n",
    "train_file = '../data/train.csv'\n",
    "target_playlists_file = '../data/target_playlists.csv'\n",
    "tracks_file = '../data/tracks.csv'\n",
    "\n",
    "# reading of all files and renaming columns\n",
    "train_data = pd.read_csv(train_file)\n",
    "train_data.columns = ['playlist_id', 'track_id']\n",
    "\n",
    "tracks_data = pd.read_csv(tracks_file)\n",
    "tracks_data.columns = ['track_id', 'album_id', 'artist_id', 'duration_sec']\n",
    "\n",
    "target_playlists = pd.read_csv(target_playlists_file)\n",
    "target_playlists.columns = ['playlist_id']\n",
    "\n",
    "# building the URM matrix\n",
    "grouped_playlists = train_data.groupby('playlist_id', as_index=True).apply(lambda x: list(x['track_id']))\n",
    "URM = MultiLabelBinarizer(sparse_output=True).fit_transform(grouped_playlists)\n",
    "URM_csr = URM.tocsr()\n",
    "\n",
    "# building the ICM matrix\n",
    "artists = tracks_data.reindex(columns=['track_id', 'artist_id'])\n",
    "artists.sort_values(by='track_id', inplace=True) # this seems not useful, values are already ordered\n",
    "artists_list = [[a] for a in artists['artist_id']]\n",
    "icm_artists = MultiLabelBinarizer(sparse_output=True).fit_transform(artists_list)\n",
    "icm_artists_csr = icm_artists.tocsr()\n",
    "\n",
    "albums = tracks_data.reindex(columns=['track_id', 'album_id'])\n",
    "albums.sort_values(by='track_id', inplace=True) # this seems not useful, values are already ordered\n",
    "albums_list = [[a] for a in albums['album_id']]\n",
    "icm_albums = MultiLabelBinarizer(sparse_output=True).fit_transform(albums_list)\n",
    "icm_albums_csr = icm_albums.tocsr()\n",
    "\n",
    "durations = tracks_data.reindex(columns=['track_id', 'duration_sec'])\n",
    "durations.sort_values(by='track_id', inplace=True) # this seems not useful, values are already ordered\n",
    "durations_list = [[d] for d in durations['duration_sec']]\n",
    "icm_durations = MultiLabelBinarizer(sparse_output=True).fit_transform(durations_list)\n",
    "icm_durations_csr = icm_durations.tocsr()\n",
    "\n",
    "ICM = sc.sparse.hstack((icm_albums_csr, icm_artists_csr, icm_durations_csr))\n",
    "ICM_csr = ICM.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_output_file():\n",
    "    file = open(\"submission.csv\", 'a')\n",
    "    file.write(\"playlist_id,track_ids\" + '\\n')\n",
    "    return file\n",
    "\n",
    "# useful to print to file with the right structure\n",
    "def print_to_file(playlist, tracks, file):\n",
    "    file.write(str(playlist) + ',')\n",
    "    index = 0\n",
    "    while index < 9:\n",
    "        file.write(str(tracks[index]) + ' ')\n",
    "        index += 1\n",
    "    file.write(str(tracks[index]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender(object):\n",
    "    \n",
    "    def __init__(self, URM, ICM):\n",
    "        self.URM = URM\n",
    "        self.ICM = ICM\n",
    "        \n",
    "            \n",
    "    def fit(self, topK=50, shrink=100, normalize = True, similarity = \"tversky\"):\n",
    "        \n",
    "        similarity_object_content_based = Compute_Similarity_Python(self.ICM.T, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "        \n",
    "        similarity_object_user_cf = Compute_Similarity_Python(self.URM.T, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "        \n",
    "        similarity_object_item_cf = Compute_Similarity_Python(self.URM, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "    \n",
    "        self.W_sparse_content_based = similarity_object_content_based.compute_similarity()\n",
    "        print(np.shape(self.W_sparse_content_based))\n",
    "        \"\"\"\n",
    "        self.W_sparse_user_cf = similarity_object_user_cf.compute_similarity()\n",
    "        print(np.shape(self.W_sparse_user_cf))\n",
    "        \"\"\"\n",
    "        self.W_sparse_item_cf = similarity_object_item_cf.compute_similarity()\n",
    "        print(np.shape(self.W_sparse_item_cf))\n",
    "        \n",
    "    def recommend(self, user_id, alfa, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        \n",
    "        scores_content_based = user_profile.dot(self.W_sparse_content_based).toarray().ravel()\n",
    "        #scores_user_cf = user_id.dot(self.W_sparse_user_cf).toarray().ravel()\n",
    "        scores_item_cf = user_profile.dot(self.W_sparse_item_cf).toarray().ravel()\n",
    "        \n",
    "        #print (\"score item cf\")\n",
    "        #print (np.shape(scores_item_cf))\n",
    "        \n",
    "        # print (\"score user cf\")\n",
    "        # print (np.shape(scores_user_cf))\n",
    "        \n",
    "        scores = alfa * scores_content_based + (1 - alfa) * scores_item_cf\n",
    "        \n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "\n",
    "def recall(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "\n",
    "def MAP(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "\n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, alfa, at=10):\n",
    "\n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "\n",
    "    num_eval = 0\n",
    "\n",
    "    URM_test = sc.sparse.csr_matrix(URM_test)\n",
    "\n",
    "    n_users = URM_test.shape[0]\n",
    "\n",
    "\n",
    "    for user_id in range(n_users):\n",
    "\n",
    "        if user_id % 10000 == 0:\n",
    "            print(\"Evaluated user {} of {}\".format(user_id, n_users))\n",
    "\n",
    "        start_pos = URM_test.indptr[user_id]\n",
    "        end_pos = URM_test.indptr[user_id+1]\n",
    "\n",
    "        if end_pos-start_pos>0:\n",
    "\n",
    "            relevant_items = URM_test.indices[start_pos:end_pos]\n",
    "\n",
    "            recommended_items = recommender_object.recommend(user_id, alfa, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "            cumulative_precision += precision(is_relevant, relevant_items)\n",
    "            cumulative_recall += recall(is_relevant, relevant_items)\n",
    "            cumulative_MAP += MAP(is_relevant, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "\n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "\n",
    "    result_dict = {\n",
    "        \"precision\": cumulative_precision,\n",
    "        \"recall\": cumulative_recall,\n",
    "        \"MAP\": cumulative_MAP,\n",
    "    }\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_splitter import train_test_holdout\n",
    "\n",
    "\n",
    "URM_train, URM_test = train_test_holdout(URM_csr, train_perc = 0.8)\n",
    "\n",
    "\n",
    "from Compute_Similarity_Python import Compute_Similarity_Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50446, 20635)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(URM_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50446, 20635)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(URM_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50446, 20635)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(URM_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity column 20600 ( 100 % ), 1931.32 column/sec, elapsed time 0.18 min\n",
      "(20635, 20635)\n",
      "Similarity column 20600 ( 100 % ), 860.26 column/sec, elapsed time 0.40 min\n",
      "(20635, 20635)\n"
     ]
    }
   ],
   "source": [
    "recommender = Recommender(URM_train, ICM_csr)\n",
    "recommender.fit(shrink=5, topK=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated user 0 of 50446\n",
      "Evaluated user 10000 of 50446\n",
      "Evaluated user 20000 of 50446\n",
      "Evaluated user 30000 of 50446\n",
      "Evaluated user 40000 of 50446\n",
      "Evaluated user 50000 of 50446\n",
      "Recommender performance is: Precision = 0.1706, Recall = 0.1550, MAP = 0.1241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.1705933769046123,\n",
       " 'recall': 0.1549582518579134,\n",
       " 'MAP': 0.1241464879705452}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_algorithm(URM_test, recommender, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_algorithm(URM_test, recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution of the recommendations\n",
    "file = initialize_output_file()\n",
    "\n",
    "for playlist in target_playlists.itertuples(index=True, name='Pandas'):\n",
    "    playlist_id = getattr(playlist, \"playlist_id\")\n",
    "    tracks = recommender.recommend(playlist_id, 10, True)\n",
    "    print_to_file(playlist_id, tracks, file)\n",
    "    \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
