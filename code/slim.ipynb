{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "import time\n",
    "import sys\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from data_splitter import train_test_holdout\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "# addresses of the files\n",
    "train_file = '../data/train.csv'\n",
    "target_playlists_file = '../data/target_playlists.csv'\n",
    "tracks_file = '../data/tracks.csv'\n",
    "\n",
    "# reading of all files and renaming columns\n",
    "train_data = pd.read_csv(train_file)\n",
    "train_data.columns = ['playlist_id', 'track_id']\n",
    "\n",
    "tracks_data = pd.read_csv(tracks_file)\n",
    "tracks_data.columns = ['track_id', 'album_id', 'artist_id', 'duration_sec']\n",
    "\n",
    "target_playlists = pd.read_csv(target_playlists_file)\n",
    "target_playlists.columns = ['playlist_id']\n",
    "\n",
    "# building the URM matrix\n",
    "grouped_playlists = train_data.groupby('playlist_id', as_index=True).apply(lambda x: list(x['track_id']))\n",
    "URM = MultiLabelBinarizer(sparse_output=True).fit_transform(grouped_playlists)\n",
    "URM_csr = URM.tocsr()\n",
    "\n",
    "# building the ICM matrix\n",
    "artists = tracks_data.reindex(columns=['track_id', 'artist_id'])\n",
    "artists.sort_values(by='track_id', inplace=True) # this seems not useful, values are already ordered\n",
    "artists_list = [[a] for a in artists['artist_id']]\n",
    "icm_artists = MultiLabelBinarizer(sparse_output=True).fit_transform(artists_list)\n",
    "icm_artists_csr = icm_artists.tocsr()\n",
    "\n",
    "albums = tracks_data.reindex(columns=['track_id', 'album_id'])\n",
    "albums.sort_values(by='track_id', inplace=True) # this seems not useful, values are already ordered\n",
    "albums_list = [[a] for a in albums['album_id']]\n",
    "icm_albums = MultiLabelBinarizer(sparse_output=True).fit_transform(albums_list)\n",
    "icm_albums_csr = icm_albums.tocsr()\n",
    "\n",
    "durations = tracks_data.reindex(columns=['track_id', 'duration_sec'])\n",
    "durations.sort_values(by='track_id', inplace=True) # this seems not useful, values are already ordered\n",
    "durations_list = [[d] for d in durations['duration_sec']]\n",
    "icm_durations = MultiLabelBinarizer(sparse_output=True).fit_transform(durations_list)\n",
    "icm_durations_csr = icm_durations.tocsr()\n",
    "\n",
    "ICM = sc.sparse.hstack((icm_albums_csr, icm_artists_csr, icm_durations_csr))\n",
    "ICM_csr = ICM.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute_Similarity_Python\n",
    "\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sc.sparse.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sc.sparse.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sc.sparse.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sc.sparse.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sc.sparse.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sc.sparse.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sc.sparse.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)\n",
    "\n",
    "\n",
    "\n",
    "class Compute_Similarity_Python:\n",
    "\n",
    "\n",
    "    def __init__(self, dataMatrix, topK=100, shrink = 0, normalize = True,\n",
    "                 asymmetric_alpha = 0.5, tversky_alpha = 1.0, tversky_beta = 1.0,\n",
    "                 similarity = \"cosine\", row_weights = None):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity on the columns of dataMatrix\n",
    "        If it is computed on URM=|users|x|items|, pass the URM as is.\n",
    "        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n",
    "        :param dataMatrix:\n",
    "        :param topK:\n",
    "        :param shrink:\n",
    "        :param normalize:           If True divide the dot product by the product of the norms\n",
    "        :param row_weights:         Multiply the values in each row by a specified value. Array\n",
    "        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\n",
    "        :param similarity:  \"cosine\"        computes Cosine similarity\n",
    "                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\n",
    "                            \"asymmetric\"    computes Asymmetric Cosine\n",
    "                            \"pearson\"       computes Pearson Correlation, removing the average of the items\n",
    "                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\n",
    "                            \"dice\"          computes Dice similarity for binary interactions\n",
    "                            \"tversky\"       computes Tversky similarity for binary interactions\n",
    "                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Asymmetric Cosine as described in: \n",
    "        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        super(Compute_Similarity_Python, self).__init__()\n",
    "\n",
    "        self.TopK = topK\n",
    "        self.shrink = shrink\n",
    "        self.normalize = normalize\n",
    "        self.n_columns = dataMatrix.shape[1]\n",
    "        self.n_rows = dataMatrix.shape[0]\n",
    "        self.asymmetric_alpha = asymmetric_alpha\n",
    "        self.tversky_alpha = tversky_alpha\n",
    "        self.tversky_beta = tversky_beta\n",
    "\n",
    "        self.dataMatrix = dataMatrix.copy()\n",
    "\n",
    "        self.adjusted_cosine = False\n",
    "        self.asymmetric_cosine = False\n",
    "        self.pearson_correlation = False\n",
    "        self.tanimoto_coefficient = False\n",
    "        self.dice_coefficient = False\n",
    "        self.tversky_coefficient = False\n",
    "\n",
    "        if similarity == \"adjusted\":\n",
    "            self.adjusted_cosine = True\n",
    "        elif similarity == \"asymmetric\":\n",
    "            self.asymmetric_cosine = True\n",
    "        elif similarity == \"pearson\":\n",
    "            self.pearson_correlation = True\n",
    "        elif similarity == \"jaccard\" or similarity == \"tanimoto\":\n",
    "            self.tanimoto_coefficient = True\n",
    "            # Tanimoto has a specific kind of normalization\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"dice\":\n",
    "            self.dice_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"tversky\":\n",
    "            self.tversky_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"cosine\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Cosine_Similarity: value for paramether 'mode' not recognized.\"\n",
    "                             \" Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',\"\n",
    "                             \"dice, tversky.\"\n",
    "                             \" Passed value was '{}'\".format(similarity))\n",
    "\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_columns, self.n_columns))\n",
    "\n",
    "\n",
    "        self.use_row_weights = False\n",
    "\n",
    "        if row_weights is not None:\n",
    "\n",
    "            if dataMatrix.shape[0] != len(row_weights):\n",
    "                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\n",
    "                                 \"Col_weights has {} columns, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\n",
    "\n",
    "            self.use_row_weights = True\n",
    "            self.row_weights = row_weights.copy()\n",
    "            self.row_weights_diag = sc.diags(self.row_weights)\n",
    "\n",
    "            self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def applyAdjustedCosine(self):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding row\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n",
    "\n",
    "\n",
    "        interactionsPerRow = np.diff(self.dataMatrix.indptr)\n",
    "\n",
    "        nonzeroRows = interactionsPerRow > 0\n",
    "        sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n",
    "\n",
    "        rowAverage = np.zeros_like(sumPerRow)\n",
    "        rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n",
    "\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_row = 0\n",
    "        end_row= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_row < self.n_rows:\n",
    "\n",
    "            end_row = min(self.n_rows, end_row + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= \\\n",
    "                np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n",
    "\n",
    "            start_row += blockSize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def applyPearsonCorrelation(self):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding column\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        interactionsPerCol = np.diff(self.dataMatrix.indptr)\n",
    "\n",
    "        nonzeroCols = interactionsPerCol > 0\n",
    "        sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n",
    "\n",
    "        colAverage = np.zeros_like(sumPerCol)\n",
    "        colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n",
    "\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_col = 0\n",
    "        end_col= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_col < self.n_columns:\n",
    "\n",
    "            end_col = min(self.n_columns, end_col + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= \\\n",
    "                np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n",
    "\n",
    "            start_col += blockSize\n",
    "\n",
    "\n",
    "    def useOnlyBooleanInteractions(self):\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_pos = 0\n",
    "        end_pos= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_pos < len(self.dataMatrix.data):\n",
    "\n",
    "            end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos-start_pos)\n",
    "\n",
    "            start_pos += blockSize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_similarity(self, start_col=None, end_col=None, block_size = 100):\n",
    "        \"\"\"\n",
    "        Compute the similarity for the given dataset\n",
    "        :param self:\n",
    "        :param start_col: column to begin with\n",
    "        :param end_col: column to stop before, end_col is excluded\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        values = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_print_batch = start_time\n",
    "        processedItems = 0\n",
    "\n",
    "\n",
    "        if self.adjusted_cosine:\n",
    "            self.applyAdjustedCosine()\n",
    "\n",
    "        elif self.pearson_correlation:\n",
    "            self.applyPearsonCorrelation()\n",
    "\n",
    "        elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n",
    "            self.useOnlyBooleanInteractions()\n",
    "\n",
    "\n",
    "        # We explore the matrix column-wise\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        # Compute sum of squared values to be used in normalization\n",
    "        sumOfSquared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n",
    "\n",
    "        # Tanimoto does not require the square root to be applied\n",
    "        if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n",
    "            sumOfSquared = np.sqrt(sumOfSquared)\n",
    "\n",
    "        if self.asymmetric_cosine:\n",
    "            sumOfSquared_to_1_minus_alpha = sumOfSquared.power(2 * (1 - self.asymmetric_alpha))\n",
    "            sumOfSquared_to_alpha = sumOfSquared.power(2 * self.asymmetric_alpha)\n",
    "\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "        start_col_local = 0\n",
    "        end_col_local = self.n_columns\n",
    "\n",
    "        if start_col is not None and start_col>0 and start_col<self.n_columns:\n",
    "            start_col_local = start_col\n",
    "\n",
    "        if end_col is not None and end_col>start_col_local and end_col<self.n_columns:\n",
    "            end_col_local = end_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start_col_block = start_col_local\n",
    "\n",
    "        this_block_size = 0\n",
    "\n",
    "        # Compute all similarities for each item using vectorization\n",
    "        while start_col_block < end_col_local:\n",
    "\n",
    "            # Add previous block size\n",
    "            processedItems += this_block_size\n",
    "\n",
    "            end_col_block = min(start_col_block + block_size, end_col_local)\n",
    "            this_block_size = end_col_block-start_col_block\n",
    "\n",
    "\n",
    "            if time.time() - start_time_print_batch >= 30 or end_col_block==end_col_local:\n",
    "                columnPerSec = processedItems / (time.time() - start_time)\n",
    "\n",
    "                print(\"Similarity column {} ( {:2.0f} % ), {:.2f} column/sec, elapsed time {:.2f} min\".format(\n",
    "                    processedItems, processedItems / (end_col_local - start_col_local) * 100, columnPerSec, (time.time() - start_time)/ 60))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_print_batch = time.time()\n",
    "\n",
    "\n",
    "            # All data points for a given item\n",
    "            item_data = self.dataMatrix[:, start_col_block:end_col_block]\n",
    "            item_data = item_data.toarray().squeeze()\n",
    "\n",
    "            if self.use_row_weights:\n",
    "                #item_data = np.multiply(item_data, self.row_weights)\n",
    "                #item_data = item_data.T.dot(self.row_weights_diag).T\n",
    "                this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n",
    "\n",
    "            else:\n",
    "                # Compute item similarities\n",
    "                this_block_weights = self.dataMatrix.T.dot(item_data)\n",
    "\n",
    "\n",
    "\n",
    "            for col_index_in_block in range(this_block_size):\n",
    "\n",
    "                if this_block_size == 1:\n",
    "                    this_column_weights = this_block_weights\n",
    "                else:\n",
    "                    this_column_weights = this_block_weights[:,col_index_in_block]\n",
    "\n",
    "\n",
    "                columnIndex = col_index_in_block + start_col_block\n",
    "                this_column_weights[columnIndex] = 0.0\n",
    "\n",
    "                # Apply normalization and shrinkage, ensure denominator != 0\n",
    "                if self.normalize:\n",
    "\n",
    "                    if self.asymmetric_cosine:\n",
    "                        denominator = sumOfSquared_to_alpha[columnIndex] * sumOfSquared_to_1_minus_alpha + self.shrink + 1e-6\n",
    "                    else:\n",
    "                        denominator = sumOfSquared[columnIndex] * sumOfSquared + self.shrink + 1e-6\n",
    "\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "\n",
    "                # Apply the specific denominator for Tanimoto\n",
    "                elif self.tanimoto_coefficient:\n",
    "                    denominator = sumOfSquared[columnIndex] + sumOfSquared - this_column_weights + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                elif self.dice_coefficient:\n",
    "                    denominator = sumOfSquared[columnIndex] + sumOfSquared + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                elif self.tversky_coefficient:\n",
    "                    denominator = this_column_weights + \\\n",
    "                                  (sumOfSquared[columnIndex] - this_column_weights)*self.tversky_alpha + \\\n",
    "                                  (sumOfSquared - this_column_weights)*self.tversky_beta + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                # If no normalization or tanimoto is selected, apply only shrink\n",
    "                elif self.shrink != 0:\n",
    "                    this_column_weights = this_column_weights/self.shrink\n",
    "\n",
    "\n",
    "                #this_column_weights = this_column_weights.toarray().ravel()\n",
    "\n",
    "                if self.TopK == 0:\n",
    "                    self.W_dense[:, columnIndex] = this_column_weights\n",
    "\n",
    "                else:\n",
    "                    # Sort indices and select TopK\n",
    "                    # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                    # - Partition the data to extract the set of relevant items\n",
    "                    # - Sort only the relevant items\n",
    "                    # - Get the original item index\n",
    "                    relevant_items_partition = (-this_column_weights).argpartition(self.TopK-1)[0:self.TopK]\n",
    "                    relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n",
    "                    top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n",
    "\n",
    "                    # Incrementally build sparse matrix, do not add zeros\n",
    "                    notZerosMask = this_column_weights[top_k_idx] != 0.0\n",
    "                    numNotZeros = np.sum(notZerosMask)\n",
    "\n",
    "                    values.extend(this_column_weights[top_k_idx][notZerosMask])\n",
    "                    rows.extend(top_k_idx[notZerosMask])\n",
    "                    cols.extend(np.ones(numNotZeros) * columnIndex)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            start_col_block += block_size\n",
    "\n",
    "        # End while on columns\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            return self.W_dense\n",
    "\n",
    "        else:\n",
    "\n",
    "            W_sparse = sc.sparse.csr_matrix((values, (rows, cols)),\n",
    "                                      shape=(self.n_columns, self.n_columns),\n",
    "                                      dtype=np.float32)\n",
    "\n",
    "\n",
    "            return W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_recommender(object):\n",
    "    \n",
    "    def __init__(self, URM, ICM):\n",
    "        self.URM = URM\n",
    "        self.ICM = ICM\n",
    "        \n",
    "            \n",
    "    def fit_content_based(self, topK=50, shrink=100, normalize = True, similarity = \"cosine\"):\n",
    "        \n",
    "        similarity_object_content_based = Compute_Similarity_Python(self.ICM.T, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "    \n",
    "        self.W_sparse_content_based = similarity_object_content_based.compute_similarity()\n",
    "    \n",
    "    def fit_item_based(self, topK=50, shrink=100, normalize = True, similarity = \"cosine\"):\n",
    "        \n",
    "        similarity_object_item_cf = Compute_Similarity_Python(self.URM, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize, \n",
    "                                                  similarity = similarity)\n",
    "    \n",
    "        self.W_sparse_item_cf = similarity_object_item_cf.compute_similarity()\n",
    "    \n",
    "    def fit_user_based(self, topK=50, shrink=100, normalize = True, similarity = \"cosine\"):\n",
    "        \n",
    "        similarity_object_user_based = Compute_Similarity_Python(self.URM.T, shrink=shrink, \n",
    "                                                                topK=topK, normalize=normalize,\n",
    "                                                                similarity=similarity)\n",
    "        \n",
    "        self.W_sparse_user_cf = similarity_object_user_based.compute_similarity()\n",
    "    \n",
    "    def recommend(self, user_id, alfa, beta, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        user_profile_ub = self.W_sparse_user_cf[user_id]\n",
    "        \n",
    "        scores_content_based = user_profile.dot(self.W_sparse_content_based).toarray().ravel()\n",
    "        scores_item_cf = user_profile.dot(self.W_sparse_item_cf).toarray().ravel()\n",
    "        scores_user_cf = user_profile_ub.dot(self.URM).toarray().ravel()\n",
    "        \n",
    "        scores = (1 - alfa - beta) * scores_content_based + alfa * scores_item_cf + beta * scores_user_cf\n",
    "        \n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "from tqdm import tqdm\n",
    "def precision(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "\n",
    "def recall(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "\n",
    "def MAP(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "\n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, target, alfa, beta, at=10):\n",
    "\n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "\n",
    "    num_eval = 0\n",
    "\n",
    "    URM_test = sc.sparse.csr_matrix(URM_test)\n",
    "\n",
    "    n_users = URM_test.shape[0]\n",
    "\n",
    "\n",
    "    for user_id in tqdm(target):\n",
    "\n",
    "        if user_id % 10000 == 0:\n",
    "            print(\"Evaluated user {} of {}\".format(user_id, n_users))\n",
    "\n",
    "        start_pos = URM_test.indptr[user_id]\n",
    "        end_pos = URM_test.indptr[user_id+1]\n",
    "\n",
    "        if end_pos-start_pos>0:\n",
    "\n",
    "            relevant_items = URM_test.indices[start_pos:end_pos]\n",
    "            \n",
    "            recommended_items = recommender_object.recommend(user_id, alfa, beta, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "            cumulative_precision += precision(is_relevant, relevant_items)\n",
    "            cumulative_recall += recall(is_relevant, relevant_items)\n",
    "            cumulative_MAP += MAP(is_relevant, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "\n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "\n",
    "    result_dict = {\n",
    "        \"precision\": cumulative_precision,\n",
    "        \"recall\": cumulative_recall,\n",
    "        \"MAP\": cumulative_MAP,\n",
    "    }\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_train, URM_test = train_test_holdout(URM_csr, train_perc = 0.8)\n",
    "\n",
    "# to avoid dimension mismatch due to the random nature of the split\n",
    "t1 = np.shape(URM_train)\n",
    "t2 = np.shape(URM_test)\n",
    "while(t1 != t2):\n",
    "    URM_train, URM_test = train_test_holdout(URM_csr, train_perc = 0.8)\n",
    "    t1 = np.shape(URM_train)\n",
    "    t2 = np.shape(URM_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target_playlists[\"playlist_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "class SLIM_BPR_Recommender(object):\n",
    "\n",
    "    def __init__(self, URM, lambda_i = 0.0025, lambda_j = 0.00025, learning_rate = 0.05):\n",
    "        super(SLIM_BPR_Recommender, self).__init__()\n",
    "\n",
    "        self.URM = URM\n",
    "        self.n_users = URM.shape[0]\n",
    "        self.n_items = URM.shape[1]\n",
    "        self.lambda_i = lambda_i\n",
    "        self.lambda_j = lambda_j\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.normalize = False\n",
    "        self.sparse_weights = False\n",
    "\n",
    "\n",
    "    def updateFactors(self, user_id, pos_item_id, neg_item_id):\n",
    "\n",
    "        # Calculate current predicted score\n",
    "        userSeenItems = self.URM[user_id].indices\n",
    "        prediction = 0\n",
    "\n",
    "        for userSeenItem in userSeenItems:\n",
    "            prediction += self.S[pos_item_id, userSeenItem] - self.S[neg_item_id, userSeenItem]\n",
    "\n",
    "\n",
    "        x_uij = prediction\n",
    "        logisticFunction = expit(-x_uij)\n",
    "\n",
    "        # Update similarities for all items except those sampled\n",
    "        for userSeenItem in userSeenItems:\n",
    "\n",
    "            # For positive item is PLUS logistic minus lambda*S\n",
    "            if(pos_item_id != userSeenItem):\n",
    "                update = logisticFunction - self.lambda_i*self.S[pos_item_id, userSeenItem]\n",
    "                self.S[pos_item_id, userSeenItem] += self.learning_rate*update\n",
    "\n",
    "            # For positive item is MINUS logistic minus lambda*S\n",
    "            if (neg_item_id != userSeenItem):\n",
    "                update = - logisticFunction - self.lambda_j*self.S[neg_item_id, userSeenItem]\n",
    "                self.S[neg_item_id, userSeenItem] += self.learning_rate*update\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, epochs=15):\n",
    "        \"\"\"\n",
    "        Train SLIM wit BPR. If the model was already trained, overwrites matrix S\n",
    "        :param epochs:\n",
    "        :return: -\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize similarity with random values and zero-out diagonal\n",
    "        self.S = np.random.random((self.n_items, self.n_items)).astype('float32')\n",
    "        self.S[np.arange(self.n_items),np.arange(self.n_items)] = 0\n",
    "\n",
    "        start_time_train = time.time()\n",
    "\n",
    "        for currentEpoch in range(epochs):\n",
    "\n",
    "            start_time_epoch = time.time()\n",
    "\n",
    "            self.epochIteration()\n",
    "            print(\"Epoch {} of {} complete in {:.2f} minutes\".format(currentEpoch+1, epochs, float(time.time()-start_time_epoch)/60))\n",
    "\n",
    "        print(\"Train completed in {:.2f} minutes\".format(float(time.time()-start_time_train)/60))\n",
    "\n",
    "        # The similarity matrix is learnt row-wise\n",
    "        # To be used in the product URM*S must be transposed to be column-wise\n",
    "        self.W = sps.csr_matrix(self.S.T)\n",
    "        \n",
    "        del self.S\n",
    "\n",
    "\n",
    "    def epochIteration(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        numPositiveIteractions = self.URM.nnz\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Uniform user sampling without replacement\n",
    "        for numSample in range(numPositiveIteractions):\n",
    "\n",
    "            user_id, pos_item_id, neg_item_id = self.sampleTriple()\n",
    "            self.updateFactors(user_id, pos_item_id, neg_item_id)\n",
    "\n",
    "            if(numSample % 100000 == 0):\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.4f} seconds\".format(numSample,\n",
    "                                  100.0* float(numSample)/numPositiveIteractions,\n",
    "                                  time.time()-start_time))\n",
    "\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def sampleUser(self):\n",
    "        \"\"\"\n",
    "        Sample a user that has viewed at least one and not all items\n",
    "        :return: user_id\n",
    "        \"\"\"\n",
    "        while(True):\n",
    "\n",
    "            user_id = np.random.randint(0, self.n_users)\n",
    "            numSeenItems = self.URM[user_id].nnz\n",
    "\n",
    "            if(numSeenItems >0 and numSeenItems<self.n_items):\n",
    "                return user_id\n",
    "\n",
    "\n",
    "\n",
    "    def sampleItemPair(self, user_id):\n",
    "        \"\"\"\n",
    "        Returns for the given user a random seen item and a random not seen item\n",
    "        :param user_id:\n",
    "        :return: pos_item_id, neg_item_id\n",
    "        \"\"\"\n",
    "\n",
    "        userSeenItems = self.URM[user_id].indices\n",
    "\n",
    "        pos_item_id = userSeenItems[np.random.randint(0,len(userSeenItems))]\n",
    "\n",
    "        while(True):\n",
    "\n",
    "            neg_item_id = np.random.randint(0, self.n_items)\n",
    "\n",
    "            if(neg_item_id not in userSeenItems):\n",
    "                return pos_item_id, neg_item_id\n",
    "\n",
    "\n",
    "    def sampleTriple(self):\n",
    "        \"\"\"\n",
    "        Randomly samples a user and then samples randomly a seen and not seen item\n",
    "        :return: user_id, pos_item_id, neg_item_id\n",
    "        \"\"\"\n",
    "\n",
    "        user_id = self.sampleUser()\n",
    "        pos_item_id, neg_item_id = self.sampleItemPair(user_id)\n",
    "\n",
    "        return user_id, pos_item_id, neg_item_id\n",
    "     \n",
    "    \n",
    "    def recommend(self, user_id, alfa=0, beta=0, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.W).toarray().ravel()\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 ( 0.00% ) in 0.0625 seconds\n",
      "Processed 100000 ( 16.49% ) in 28.1446 seconds\n",
      "Processed 200000 ( 32.98% ) in 26.3063 seconds\n",
      "Processed 300000 ( 49.47% ) in 30.7393 seconds\n",
      "Processed 400000 ( 65.96% ) in 31.7510 seconds\n",
      "Processed 500000 ( 82.45% ) in 30.6022 seconds\n",
      "Processed 600000 ( 98.94% ) in 27.6383 seconds\n",
      "Epoch 1 of 1 complete in 2.95 minutes\n",
      "Train completed in 2.95 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/10000 [00:00<?, ?it/s]\n",
      "  0%|                                                                              | 1/10000 [00:03<9:19:42,  3.36s/it]\n",
      "  0%|                                                                              | 2/10000 [00:05<8:14:03,  2.96s/it]\n",
      "  0%|                                                                              | 3/10000 [00:07<7:14:49,  2.61s/it]\n",
      "  0%|                                                                              | 4/10000 [00:08<6:23:58,  2.30s/it]\n",
      "  0%|                                                                              | 5/10000 [00:10<5:49:10,  2.10s/it]\n",
      "  0%|                                                                              | 6/10000 [00:12<5:26:21,  1.96s/it]\n",
      "  0%|                                                                              | 7/10000 [00:13<5:10:22,  1.86s/it]\n",
      "  0%|                                                                              | 8/10000 [00:15<4:59:11,  1.80s/it]\n",
      "  0%|                                                                              | 9/10000 [00:16<4:52:07,  1.75s/it]\n",
      "  0%|                                                                             | 10/10000 [00:18<4:44:50,  1.71s/it]\n",
      "  0%|                                                                             | 11/10000 [00:20<4:40:30,  1.68s/it]\n",
      "  0%|                                                                             | 12/10000 [00:21<4:38:15,  1.67s/it]\n",
      "  0%|                                                                             | 13/10000 [00:23<4:36:39,  1.66s/it]\n",
      "  0%|                                                                             | 14/10000 [00:25<4:33:58,  1.65s/it]\n",
      "  0%|                                                                             | 15/10000 [00:26<4:33:39,  1.64s/it]\n",
      "  0%|                                                                             | 16/10000 [00:28<4:33:25,  1.64s/it]\n",
      "  0%|▏                                                                            | 17/10000 [00:30<4:33:14,  1.64s/it]\n",
      "  0%|▏                                                                            | 18/10000 [00:31<4:33:07,  1.64s/it]\n",
      "  0%|▏                                                                            | 19/10000 [00:33<4:33:01,  1.64s/it]\n",
      "  0%|▏                                                                            | 20/10000 [00:34<4:34:30,  1.65s/it]\n",
      "  0%|▏                                                                            | 21/10000 [00:36<4:33:58,  1.65s/it]\n",
      "  0%|▏                                                                            | 22/10000 [00:38<4:32:48,  1.64s/it]\n",
      "  0%|▏                                                                            | 23/10000 [00:39<4:34:19,  1.65s/it]\n",
      "  0%|▏                                                                            | 24/10000 [00:41<4:37:43,  1.67s/it]\n",
      "  0%|▏                                                                            | 25/10000 [00:43<4:37:45,  1.67s/it]\n",
      "  0%|▏                                                                            | 26/10000 [00:45<4:43:13,  1.70s/it]\n",
      "  0%|▏                                                                            | 27/10000 [00:46<4:39:14,  1.68s/it]"
     ]
    }
   ],
   "source": [
    "# slim recommender\n",
    "rec = SLIM_BPR_Recommender(URM_train)\n",
    "rec.fit(epochs=1)\n",
    "evaluate_algorithm(URM_test, rec, target, alfa=0, beta=0, at=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity column 20600 ( 100 % ), 2691.23 column/sec, elapsed time 0.13 min\n",
      "Similarity column 20600 ( 100 % ), 855.19 column/sec, elapsed time 0.40 min\n",
      "Similarity column 29400 ( 58 % ), 979.89 column/sec, elapsed time 0.50 min\n",
      "Similarity column 50400 ( 100 % ), 960.26 column/sec, elapsed time 0.87 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:41<00:00, 62.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommender performance is: Precision = 0.1610, Recall = 0.1530, MAP = 0.1100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.1609621924384837,\n",
       " 'recall': 0.15299742265342564,\n",
       " 'MAP': 0.10997209501109016}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the hybrid recommender\n",
    "recommender = Hybrid_recommender(URM_train, ICM_csr)\n",
    "recommender.fit_content_based(shrink=5, topK=100)\n",
    "recommender.fit_item_based(shrink=5, topK=100)\n",
    "recommender.fit_user_based(shrink=5, topK=100)\n",
    "evaluate_algorithm(URM_test, recommender,target, 0.43, 0.37, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
