{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addresses of the files\n",
    "train_file = '../data/train.csv'\n",
    "target_playlists_file = '../data/target_playlists.csv'\n",
    "tracks_file = '../data/tracks.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading of all files and renaming columns\n",
    "train_data = pd.read_csv(train_file)\n",
    "train_data.columns = ['playlist_id', 'track_id']\n",
    "\n",
    "tracks_data = pd.read_csv(tracks_file)\n",
    "tracks_data.columns = ['track_id', 'album_id', 'artist_id', 'duration_sec']\n",
    "\n",
    "target_playlists = pd.read_csv(target_playlists_file)\n",
    "target_playlists.columns = ['playlist_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the URM matrix\n",
    "grouped_playlists = train_data.groupby('playlist_id', as_index=True).apply(lambda x: list(x['track_id']))\n",
    "URM = MultiLabelBinarizer(sparse_output=True).fit_transform(grouped_playlists)\n",
    "URM_csr = URM.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the ICM matrix\n",
    "artists = tracks_data.reindex(columns=['track_id', 'artist_id'])\n",
    "artists.sort_values(by='track_id', inplace=True) # this seems not useful, values are already oredered\n",
    "artists_list = [[a] for a in artists['artist_id']]\n",
    "icm_artists = MultiLabelBinarizer(sparse_output=True).fit_transform(artists_list)\n",
    "icm_artists_csr = icm_artists.tocsr()\n",
    "\n",
    "albums = tracks_data.reindex(columns=['track_id', 'album_id'])\n",
    "albums.sort_values(by='track_id', inplace=True) # this seems not useful, values are already oredered\n",
    "albums_list = [[a] for a in albums['album_id']]\n",
    "icm_albums = MultiLabelBinarizer(sparse_output=True).fit_transform(albums_list)\n",
    "icm_albums_csr = icm_albums.tocsr()\n",
    "\n",
    "durations = tracks_data.reindex(columns=['track_id', 'duration_sec'])\n",
    "durations.sort_values(by='track_id', inplace=True) # this seems not useful, values are already oredered\n",
    "durations_list = [[d] for d in durations['duration_sec']]\n",
    "icm_durations = MultiLabelBinarizer(sparse_output=True).fit_transform(durations_list)\n",
    "icm_durations_csr = icm_durations.tocsr()\n",
    "\n",
    "ICM = sc.sparse.hstack((icm_albums_csr, icm_artists_csr, icm_durations_csr))\n",
    "ICM_csr = ICM.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of artists, may be useful\n",
    "artists = [a for a in artists['artist_id']]\n",
    "artists_unique = list(set(artists))\n",
    "artists_unique_and_sorted = artists_unique.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the list of target playlists\n",
    "target_list = [p for p in target_playlists['playlist_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting URM into train and set\n",
    "URM_train, URM_test = train_test_split(URM,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sc.sparse.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sc.sparse.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sc.sparse.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sc.sparse.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sc.sparse.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sc.sparse.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sc.sparse.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compute_Similarity_Python:\n",
    "\n",
    "\n",
    "    def __init__(self, dataMatrix, topK=100, shrink = 0, normalize = True,\n",
    "                 asymmetric_alpha = 0.5, tversky_alpha = 1.0, tversky_beta = 1.0,\n",
    "                 similarity = \"cosine\", row_weights = None):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity on the columns of dataMatrix\n",
    "        If it is computed on URM=|users|x|items|, pass the URM as is.\n",
    "        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n",
    "        :param dataMatrix:\n",
    "        :param topK:\n",
    "        :param shrink:\n",
    "        :param normalize:           If True divide the dot product by the product of the norms\n",
    "        :param row_weights:         Multiply the values in each row by a specified value. Array\n",
    "        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\n",
    "        :param similarity:  \"cosine\"        computes Cosine similarity\n",
    "                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\n",
    "                            \"asymmetric\"    computes Asymmetric Cosine\n",
    "                            \"pearson\"       computes Pearson Correlation, removing the average of the items\n",
    "                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\n",
    "                            \"dice\"          computes Dice similarity for binary interactions\n",
    "                            \"tversky\"       computes Tversky similarity for binary interactions\n",
    "                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Asymmetric Cosine as described in: \n",
    "        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        super(Compute_Similarity_Python, self).__init__()\n",
    "\n",
    "        self.TopK = topK\n",
    "        self.shrink = shrink\n",
    "        self.normalize = normalize\n",
    "        self.n_columns = dataMatrix.shape[1]\n",
    "        self.n_rows = dataMatrix.shape[0]\n",
    "        self.asymmetric_alpha = asymmetric_alpha\n",
    "        self.tversky_alpha = tversky_alpha\n",
    "        self.tversky_beta = tversky_beta\n",
    "\n",
    "        self.dataMatrix = dataMatrix.copy()\n",
    "\n",
    "        self.adjusted_cosine = False\n",
    "        self.asymmetric_cosine = False\n",
    "        self.pearson_correlation = False\n",
    "        self.tanimoto_coefficient = False\n",
    "        self.dice_coefficient = False\n",
    "        self.tversky_coefficient = False\n",
    "\n",
    "        if similarity == \"adjusted\":\n",
    "            self.adjusted_cosine = True\n",
    "        elif similarity == \"asymmetric\":\n",
    "            self.asymmetric_cosine = True\n",
    "        elif similarity == \"pearson\":\n",
    "            self.pearson_correlation = True\n",
    "        elif similarity == \"jaccard\" or similarity == \"tanimoto\":\n",
    "            self.tanimoto_coefficient = True\n",
    "            # Tanimoto has a specific kind of normalization\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"dice\":\n",
    "            self.dice_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"tversky\":\n",
    "            self.tversky_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"cosine\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Cosine_Similarity: value for paramether 'mode' not recognized.\"\n",
    "                             \" Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',\"\n",
    "                             \"dice, tversky.\"\n",
    "                             \" Passed value was '{}'\".format(similarity))\n",
    "\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_columns, self.n_columns))\n",
    "\n",
    "\n",
    "        self.use_row_weights = False\n",
    "\n",
    "        if row_weights is not None:\n",
    "\n",
    "            if dataMatrix.shape[0] != len(row_weights):\n",
    "                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\n",
    "                                 \"Col_weights has {} columns, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\n",
    "\n",
    "            self.use_row_weights = True\n",
    "            self.row_weights = row_weights.copy()\n",
    "            self.row_weights_diag = sps.diags(self.row_weights)\n",
    "\n",
    "            self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def applyAdjustedCosine(self):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding row\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n",
    "\n",
    "\n",
    "        interactionsPerRow = np.diff(self.dataMatrix.indptr)\n",
    "\n",
    "        nonzeroRows = interactionsPerRow > 0\n",
    "        sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n",
    "\n",
    "        rowAverage = np.zeros_like(sumPerRow)\n",
    "        rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n",
    "\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_row = 0\n",
    "        end_row= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_row < self.n_rows:\n",
    "\n",
    "            end_row = min(self.n_rows, end_row + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= \\\n",
    "                np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n",
    "\n",
    "            start_row += blockSize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def applyPearsonCorrelation(self):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding column\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        interactionsPerCol = np.diff(self.dataMatrix.indptr)\n",
    "\n",
    "        nonzeroCols = interactionsPerCol > 0\n",
    "        sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n",
    "\n",
    "        colAverage = np.zeros_like(sumPerCol)\n",
    "        colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n",
    "\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_col = 0\n",
    "        end_col= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_col < self.n_columns:\n",
    "\n",
    "            end_col = min(self.n_columns, end_col + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= \\\n",
    "                np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n",
    "\n",
    "            start_col += blockSize\n",
    "\n",
    "\n",
    "    def useOnlyBooleanInteractions(self):\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_pos = 0\n",
    "        end_pos= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_pos < len(self.dataMatrix.data):\n",
    "\n",
    "            end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos-start_pos)\n",
    "\n",
    "            start_pos += blockSize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_similarity(self, start_col=None, end_col=None, block_size = 100):\n",
    "        \"\"\"\n",
    "        Compute the similarity for the given dataset\n",
    "        :param self:\n",
    "        :param start_col: column to begin with\n",
    "        :param end_col: column to stop before, end_col is excluded\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        values = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_print_batch = start_time\n",
    "        processedItems = 0\n",
    "\n",
    "\n",
    "        if self.adjusted_cosine:\n",
    "            self.applyAdjustedCosine()\n",
    "\n",
    "        elif self.pearson_correlation:\n",
    "            self.applyPearsonCorrelation()\n",
    "\n",
    "        elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n",
    "            self.useOnlyBooleanInteractions()\n",
    "\n",
    "\n",
    "        # We explore the matrix column-wise\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        # Compute sum of squared values to be used in normalization\n",
    "        sumOfSquared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n",
    "\n",
    "        # Tanimoto does not require the square root to be applied\n",
    "        if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n",
    "            sumOfSquared = np.sqrt(sumOfSquared)\n",
    "\n",
    "        if self.asymmetric_cosine:\n",
    "            sumOfSquared_to_1_minus_alpha = sumOfSquared.power(2 * (1 - self.asymmetric_alpha))\n",
    "            sumOfSquared_to_alpha = sumOfSquared.power(2 * self.asymmetric_alpha)\n",
    "\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "        start_col_local = 0\n",
    "        end_col_local = self.n_columns\n",
    "\n",
    "        if start_col is not None and start_col>0 and start_col<self.n_columns:\n",
    "            start_col_local = start_col\n",
    "\n",
    "        if end_col is not None and end_col>start_col_local and end_col<self.n_columns:\n",
    "            end_col_local = end_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start_col_block = start_col_local\n",
    "\n",
    "        this_block_size = 0\n",
    "\n",
    "        # Compute all similarities for each item using vectorization\n",
    "        while start_col_block < end_col_local:\n",
    "\n",
    "            # Add previous block size\n",
    "            processedItems += this_block_size\n",
    "\n",
    "            end_col_block = min(start_col_block + block_size, end_col_local)\n",
    "            this_block_size = end_col_block-start_col_block\n",
    "\n",
    "\n",
    "            if time.time() - start_time_print_batch >= 30 or end_col_block==end_col_local:\n",
    "                columnPerSec = processedItems / (time.time() - start_time)\n",
    "\n",
    "                print(\"Similarity column {} ( {:2.0f} % ), {:.2f} column/sec, elapsed time {:.2f} min\".format(\n",
    "                    processedItems, processedItems / (end_col_local - start_col_local) * 100, columnPerSec, (time.time() - start_time)/ 60))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_print_batch = time.time()\n",
    "\n",
    "\n",
    "            # All data points for a given item\n",
    "            item_data = self.dataMatrix[:, start_col_block:end_col_block]\n",
    "            item_data = item_data.toarray().squeeze()\n",
    "\n",
    "            if self.use_row_weights:\n",
    "                #item_data = np.multiply(item_data, self.row_weights)\n",
    "                #item_data = item_data.T.dot(self.row_weights_diag).T\n",
    "                this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n",
    "\n",
    "            else:\n",
    "                # Compute item similarities\n",
    "                this_block_weights = self.dataMatrix.T.dot(item_data)\n",
    "\n",
    "\n",
    "\n",
    "            for col_index_in_block in range(this_block_size):\n",
    "\n",
    "                if this_block_size == 1:\n",
    "                    this_column_weights = this_block_weights\n",
    "                else:\n",
    "                    this_column_weights = this_block_weights[:,col_index_in_block]\n",
    "\n",
    "\n",
    "                columnIndex = col_index_in_block + start_col_block\n",
    "                this_column_weights[columnIndex] = 0.0\n",
    "\n",
    "                # Apply normalization and shrinkage, ensure denominator != 0\n",
    "                if self.normalize:\n",
    "\n",
    "                    if self.asymmetric_cosine:\n",
    "                        denominator = sumOfSquared_to_alpha[columnIndex] * sumOfSquared_to_1_minus_alpha + self.shrink + 1e-6\n",
    "                    else:\n",
    "                        denominator = sumOfSquared[columnIndex] * sumOfSquared + self.shrink + 1e-6\n",
    "\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "\n",
    "                # Apply the specific denominator for Tanimoto\n",
    "                elif self.tanimoto_coefficient:\n",
    "                    denominator = sumOfSquared[columnIndex] + sumOfSquared - this_column_weights + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                elif self.dice_coefficient:\n",
    "                    denominator = sumOfSquared[columnIndex] + sumOfSquared + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                elif self.tversky_coefficient:\n",
    "                    denominator = this_column_weights + \\\n",
    "                                  (sumOfSquared[columnIndex] - this_column_weights)*self.tversky_alpha + \\\n",
    "                                  (sumOfSquared - this_column_weights)*self.tversky_beta + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                # If no normalization or tanimoto is selected, apply only shrink\n",
    "                elif self.shrink != 0:\n",
    "                    this_column_weights = this_column_weights/self.shrink\n",
    "\n",
    "\n",
    "                #this_column_weights = this_column_weights.toarray().ravel()\n",
    "\n",
    "                if self.TopK == 0:\n",
    "                    self.W_dense[:, columnIndex] = this_column_weights\n",
    "\n",
    "                else:\n",
    "                    # Sort indices and select TopK\n",
    "                    # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                    # - Partition the data to extract the set of relevant items\n",
    "                    # - Sort only the relevant items\n",
    "                    # - Get the original item index\n",
    "                    relevant_items_partition = (-this_column_weights).argpartition(self.TopK-1)[0:self.TopK]\n",
    "                    relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n",
    "                    top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n",
    "\n",
    "                    # Incrementally build sparse matrix, do not add zeros\n",
    "                    notZerosMask = this_column_weights[top_k_idx] != 0.0\n",
    "                    numNotZeros = np.sum(notZerosMask)\n",
    "\n",
    "                    values.extend(this_column_weights[top_k_idx][notZerosMask])\n",
    "                    rows.extend(top_k_idx[notZerosMask])\n",
    "                    cols.extend(np.ones(numNotZeros) * columnIndex)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            start_col_block += block_size\n",
    "\n",
    "        # End while on columns\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            return self.W_dense\n",
    "\n",
    "        else:\n",
    "\n",
    "            W_sparse = sc.sparse.csr_matrix((values, (rows, cols)),\n",
    "                                      shape=(self.n_columns, self.n_columns),\n",
    "                                      dtype=np.float32)\n",
    "\n",
    "\n",
    "            return W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity column 20000 ( 100 % ), 2650.66 column/sec, elapsed time 0.13 min\n"
     ]
    }
   ],
   "source": [
    "similarity_object = Compute_Similarity_Python(ICM_csr, shrink=100, topK=50, normalize=True, similarity='cosine')\n",
    "W_sparse = similarity_object.compute_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_seen(playlist_id, scores):\n",
    "    start_pos = URM_csr.indptr[playlist_id]\n",
    "    end_pos = URM_csr.indptr[playlist_id+1]\n",
    "\n",
    "    playlist_profile = URM_csr.indices[start_pos:end_pos]\n",
    "\n",
    "    scores[playlist_profile] = -np.inf\n",
    "\n",
    "    return scores\n",
    "\n",
    "def recommend(playlist_id, at=None, exclude_seen=True): \n",
    "    # compute the scores using the dot product\n",
    "    playlist_profile = URM_csr[playlist_id]\n",
    "    scores = playlist_profile.dot(W_sparse).toarray().ravel()\n",
    "\n",
    "    if exclude_seen:\n",
    "        scores = filter_seen(user_id, scores)\n",
    "\n",
    "    # rank items\n",
    "    ranking = scores.argsort()[::-1]\n",
    "\n",
    "    return ranking[:at]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0afebf1e4620>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mplaylist_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecommend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplaylist_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-4aa587405a1e>\u001b[0m in \u001b[0;36mrecommend\u001b[1;34m(playlist_id, at, exclude_seen)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# compute the scores using the dot product\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mplaylist_profile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mURM_csr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplaylist_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplaylist_profile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW_sparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexclude_seen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \"\"\"\n\u001b[1;32m--> 361\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 478\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "for playlist_id in target_list[0:10]:\n",
    "    print(recommend(playlist_id, at=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"submission.csv\", 'a')\n",
    "file.write(\"playlist_id,track_ids\" + '\\n')\n",
    "\n",
    "# useful to print to file with the right structure \n",
    "def print_to_file(playlist, tracks, file, n=10):\n",
    "    file.write(str(playlist) + ',')\n",
    "    index = 1\n",
    "    while index < n:\n",
    "        file.write(str(tracks[index]) + ' ')\n",
    "        index += 1\n",
    "    file.write(str(tracks[index]) + '\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
