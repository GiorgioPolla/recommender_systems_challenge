{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import scipy.sparse as sps\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# addresses of the files                               \n",
    "train_file ='../data/train.csv'\n",
    "target_playlists_file = '../data/target_playlists.csv'\n",
    "tracks_file = '../data/tracks.csv'\n",
    "\n",
    "# reading of all files and renaming columns\n",
    "train_data = pd.read_csv(train_file)\n",
    "train_data.columns = ['playlist_id', 'track_id']\n",
    "\n",
    "tracks_data = pd.read_csv(tracks_file)\n",
    "tracks_data.columns = ['track_id', 'album_id', 'artist_id', 'duration_sec']\n",
    "\n",
    "target_playlists = pd.read_csv(target_playlists_file)\n",
    "target_playlists.columns = ['playlist_id']\n",
    "\n",
    "# building the URM matrix\n",
    "grouped_playlists = train_data.groupby('playlist_id', as_index=True).apply(lambda x: list(x['track_id']))\n",
    "URM = MultiLabelBinarizer(sparse_output=True).fit_transform(grouped_playlists)\n",
    "URM_csr = URM.tocsr()\n",
    "\n",
    "# building the ICM matrix\n",
    "artists = tracks_data.reindex(columns=['track_id', 'artist_id'])\n",
    "artists.sort_values(by='track_id', inplace=True) # this seems not useful, values are already ordered\n",
    "artists_list = [[a] for a in artists['artist_id']]\n",
    "icm_artists = MultiLabelBinarizer(sparse_output=True).fit_transform(artists_list)\n",
    "icm_artists_csr = icm_artists.tocsr()\n",
    "\n",
    "albums = tracks_data.reindex(columns=['track_id', 'album_id'])\n",
    "albums.sort_values(by='track_id', inplace=True) # this seems not useful, values are already ordered\n",
    "albums_list = [[a] for a in albums['album_id']]\n",
    "icm_albums = MultiLabelBinarizer(sparse_output=True).fit_transform(albums_list)\n",
    "icm_albums_csr = icm_albums.tocsr()\n",
    "\n",
    "durations = tracks_data.reindex(columns=['track_id', 'duration_sec'])\n",
    "durations.sort_values(by='track_id', inplace=True) # this seems not useful, values are already ordered\n",
    "durations_list = [[d] for d in durations['duration_sec']]\n",
    "icm_durations = MultiLabelBinarizer(sparse_output=True).fit_transform(durations_list)\n",
    "icm_durations_csr = icm_durations.tocsr()\n",
    "\n",
    "ICM = sc.sparse.hstack((icm_albums_csr, icm_artists_csr, icm_durations_csr))\n",
    "ICM_csr = ICM.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompileError",
     "evalue": "command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2017\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.16.27023\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit status 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDistutilsExecError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, sources, output_dir, macros, include_dirs, debug, extra_preargs, extra_postargs, depends)\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mDistutilsExecError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36mspawn\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m    541\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_paths\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\distutils\\ccompiler.py\u001b[0m in \u001b[0;36mspawn\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mspawn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m         \u001b[0mspawn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdry_run\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\distutils\\spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[1;34m(cmd, search_path, verbose, dry_run)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nt'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0m_spawn_nt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdry_run\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\distutils\\spawn.py\u001b[0m in \u001b[0;36m_spawn_nt\u001b[1;34m(cmd, search_path, verbose, dry_run)\u001b[0m\n\u001b[0;32m     80\u001b[0m             raise DistutilsExecError(\n\u001b[1;32m---> 81\u001b[1;33m                   \"command %r failed with exit status %d\" % (cmd, rc))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDistutilsExecError\u001b[0m: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2017\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.16.27023\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit status 2",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCompileError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a1dd2cdfc553>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cython'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'import time, sys\\nimport numpy as np\\ncimport numpy as np\\nfrom cpython.array cimport array, clone\\nfrom libc.math cimport sqrt\\nimport scipy.sparse as sps\\n\\ndef check_matrix(X, format=\\'csc\\', dtype=np.float32):\\n    if format == \\'csc\\' and not isinstance(X, sps.csc_matrix):\\n        return X.tocsc().astype(dtype)\\n    elif format == \\'csr\\' and not isinstance(X, sps.csr_matrix):\\n        return X.tocsr().astype(dtype)\\n    elif format == \\'coo\\' and not isinstance(X, sps.coo_matrix):\\n        return X.tocoo().astype(dtype)\\n    elif format == \\'dok\\' and not isinstance(X, sps.dok_matrix):\\n        return X.todok().astype(dtype)\\n    elif format == \\'bsr\\' and not isinstance(X, sps.bsr_matrix):\\n        return X.tobsr().astype(dtype)\\n    elif format == \\'dia\\' and not isinstance(X, sps.dia_matrix):\\n        return X.todia().astype(dtype)\\n    elif format == \\'lil\\' and not isinstance(X, sps.lil_matrix):\\n        return X.tolil().astype(dtype)\\n    else:\\n        return X.astype(dtype)\\n\\n    \\n\\nimport time, sys\\nimport numpy as np\\ncimport numpy as np\\nfrom cpython.array cimport array, clone\\nfrom libc.math cimport sqrt\\nimport scipy.sparse as sps\\n\\ncdef class Compute_Similarity_Cython:\\n\\n    cdef int TopK\\n    cdef long n_columns, n_rows\\n    cdef double[:] this_item_weights\\n    cdef int[:] this_item_weights_mask, this_item_weights_id\\n    cdef int this_item_weights_counter\\n    cdef int[:] user_to_item_row_ptr, user_to_item_cols\\n    cdef int[:] item_to_user_rows, item_to_user_col_ptr\\n    cdef double[:] user_to_item_data, item_to_user_data\\n    cdef double[:] sumOfSquared, sumOfSquared_to_1_minus_alpha, sumOfSquared_to_alpha\\n    cdef int shrink, normalize, adjusted_cosine, pearson_correlation, tanimoto_coefficient, asymmetric_cosine, dice_coefficient, tversky_coefficient\\n    cdef float asymmetric_alpha, tversky_alpha, tversky_beta\\n    cdef int use_row_weights\\n    cdef double[:] row_weights\\n    cdef double[:,:] W_dense\\n\\n    def __init__(self, dataMatrix, topK = 100, shrink=5, normalize = True, row_weights = None):\\n        \"\"\"\\n        Computes the cosine similarity on the columns of dataMatrix\\n        If it is computed on URM=|users|x|items|, pass the URM as is.\\n        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\\n        :param dataMatrix:\\n        :param topK:\\n        :param shrink:\\n        :param normalize:           If True divide the dot product by the product of the norms\\n        :param row_weights:         Multiply the values in each row by a specified value. Array\\n\\n        \"\"\"\\n\\n        super(Compute_Similarity_Cython, self).__init__()\\n\\n        self.n_columns = dataMatrix.shape[1]\\n        self.n_rows = dataMatrix.shape[0]\\n        self.shrink = shrink\\n        self.normalize = normalize\\n\\n        self.TopK = min(topK, self.n_columns)\\n        self.this_item_weights = np.zeros(self.n_columns, dtype=np.float64)\\n        self.this_item_weights_id = np.zeros(self.n_columns, dtype=np.int32)\\n        self.this_item_weights_mask = np.zeros(self.n_columns, dtype=np.int32)\\n        self.this_item_weights_counter = 0\\n\\n        # Copy data to avoid altering the original object\\n        dataMatrix = dataMatrix.copy()\\n\\n        # Compute sum of squared values to be used in normalization\\n        self.sumOfSquared = np.array(dataMatrix.power(2).sum(axis=0), dtype=np.float64).ravel()\\n\\n        # Apply weight after sumOfSquared has been computed but before the matrix is\\n        # split in its inner data structures\\n        self.use_row_weights = False\\n\\n        if row_weights is not None:\\n\\n            if dataMatrix.shape[0] != len(row_weights):\\n                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\\n                                 \"Row_weights has {} rows, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\\n\\n            self.use_row_weights = True\\n            self.row_weights = np.array(row_weights, dtype=np.float64)\\n\\n        dataMatrix = check_matrix(dataMatrix, \\'csr\\')\\n\\n        self.user_to_item_row_ptr = dataMatrix.indptr\\n        self.user_to_item_cols = dataMatrix.indices\\n        self.user_to_item_data = np.array(dataMatrix.data, dtype=np.float64)\\n\\n        dataMatrix = check_matrix(dataMatrix, \\'csc\\')\\n        self.item_to_user_rows = dataMatrix.indices\\n        self.item_to_user_col_ptr = dataMatrix.indptr\\n        self.item_to_user_data = np.array(dataMatrix.data, dtype=np.float64)\\n\\n        if self.TopK == 0:\\n            self.W_dense = np.zeros((self.n_columns,self.n_columns))\\n\\n\\n    cdef int[:] getUsersThatRatedItem(self, long item_id):\\n        return self.item_to_user_rows[self.item_to_user_col_ptr[item_id]:self.item_to_user_col_ptr[item_id+1]]\\n\\n    cdef int[:] getItemsRatedByUser(self, long user_id):\\n        return self.user_to_item_cols[self.user_to_item_row_ptr[user_id]:self.user_to_item_row_ptr[user_id+1]]\\n\\n\\n    cdef computeItemSimilarities(self, long item_id_input):\\n        \"\"\"\\n    \\n        The implementation here used is:\\n        - Select the first item\\n        - Initialize a zero valued array for the similarities\\n        - Get the users who rated the first item\\n        - Loop through the users\\n        -- Given a user, get the items he rated (second item)\\n        -- Update the similarity of the items he rated\\n        \\n        \"\"\"\\n\\n        cdef long user_index, user_id, item_index, item_id, item_id_second\\n        cdef int[:] users_that_rated_item = self.getUsersThatRatedItem(item_id_input)\\n        cdef int[:] items_rated_by_user\\n        cdef double rating_item_input, rating_item_second, row_weight\\n\\n        # Clean previous item\\n        for item_index in range(self.this_item_weights_counter):\\n            item_id = self.this_item_weights_id[item_index]\\n            self.this_item_weights_mask[item_id] = False\\n            self.this_item_weights[item_id] = 0.0\\n\\n        self.this_item_weights_counter = 0\\n\\n        # Get users that rated the items\\n        for user_index in range(len(users_that_rated_item)):\\n\\n            user_id = users_that_rated_item[user_index]\\n            rating_item_input = self.item_to_user_data[self.item_to_user_col_ptr[item_id_input]+user_index]\\n\\n            if self.use_row_weights:\\n                row_weight = self.row_weights[user_id]\\n            else:\\n                row_weight = 1.0\\n\\n            # Get all items rated by that user\\n            items_rated_by_user = self.getItemsRatedByUser(user_id)\\n\\n            for item_index in range(len(items_rated_by_user)):\\n\\n                item_id_second = items_rated_by_user[item_index]\\n\\n                # Do not compute the similarity on the diagonal\\n                if item_id_second != item_id_input:\\n                    # Increment similairty\\n                    rating_item_second = self.user_to_item_data[self.user_to_item_row_ptr[user_id]+item_index]\\n\\n                    self.this_item_weights[item_id_second] += rating_item_input*rating_item_second*row_weight\\n\\n                    # Update global data structure\\n                    if not self.this_item_weights_mask[item_id_second]:\\n\\n                        self.this_item_weights_mask[item_id_second] = True\\n                        self.this_item_weights_id[self.this_item_weights_counter] = item_id_second\\n                        self.this_item_weights_counter += 1\\n\\n\\n    def compute_similarity(self, start_col=None, end_col=None):\\n        \"\"\"\\n        Compute the similarity for the given dataset\\n        :param self:\\n        :param start_col: column to begin with\\n        :param end_col: column to stop before, end_col is excluded\\n        :return:\\n        \"\"\"\\n\\n        cdef int print_block_size = 500\\n        cdef int itemIndex, innerItemIndex, item_id, local_topK\\n        cdef long long topKItemIndex\\n        cdef long long[:] top_k_idx\\n\\n        # Declare numpy data type to use vetor indexing and simplify the topK selection code\\n        cdef np.ndarray[long, ndim=1] top_k_partition, top_k_partition_sorting\\n        cdef np.ndarray[np.float64_t, ndim=1] this_item_weights_np\\n        cdef long processedItems = 0\\n\\n        # Data structure to incrementally build sparse matrix\\n        # Preinitialize max possible length\\n        cdef double[:] values = np.zeros((self.n_columns*self.TopK))\\n        cdef int[:] rows = np.zeros((self.n_columns*self.TopK,), dtype=np.int32)\\n        cdef int[:] cols = np.zeros((self.n_columns*self.TopK,), dtype=np.int32)\\n        cdef long sparse_data_pointer = 0\\n        cdef int start_col_local = 0, end_col_local = self.n_columns\\n        cdef array[double] template_zero = array(\\'d\\')\\n\\n        if start_col is not None and start_col>0 and start_col<self.n_columns:\\n            start_col_local = start_col\\n\\n        if end_col is not None and end_col>start_col_local and end_col<self.n_columns:\\n            end_col_local = end_col\\n\\n        start_time = time.time()\\n        last_print_time = start_time\\n\\n        itemIndex = start_col_local\\n\\n        # Compute all similarities for each item\\n        while itemIndex < end_col_local:\\n\\n            processedItems += 1\\n\\n            if processedItems % print_block_size==0 or processedItems==end_col_local:\\n\\n                current_time = time.time()\\n\\n                # Set block size to the number of items necessary in order to print every 30 seconds\\n                itemPerSec = processedItems/(time.time()-start_time)\\n\\n                print_block_size = int(itemPerSec*30)\\n\\n                if current_time - last_print_time > 30  or processedItems==end_col_local:\\n\\n                    print(\"Similarity column {} ( {:2.0f} % ), {:.2f} column/sec, elapsed time {:.2f} min\".format(\\n                        processedItems, processedItems*1.0/(end_col_local-start_col_local)*100, itemPerSec, (time.time()-start_time) / 60))\\n\\n                    last_print_time = current_time\\n\\n                    sys.stdout.flush()\\n                    sys.stderr.flush()\\n\\n\\n            # Computed similarities go in self.this_item_weights\\n            self.computeItemSimilarities(itemIndex)\\n\\n            # Apply normalization and shrinkage, ensure denominator != 0\\n            if self.normalize:\\n                for innerItemIndex in range(self.n_columns):\\n\\n                    if self.asymmetric_cosine:\\n                        self.this_item_weights[innerItemIndex] /= self.sumOfSquared_to_alpha[itemIndex] * self.sumOfSquared_to_1_minus_alpha[innerItemIndex]\\\\\\n                                                             + self.shrink + 1e-6\\n\\n                    else:\\n                        self.this_item_weights[innerItemIndex] /= self.sumOfSquared[itemIndex] * self.sumOfSquared[innerItemIndex]\\\\\\n                                                             + self.shrink + 1e-6\\n\\n            elif self.shrink != 0:\\n                for innerItemIndex in range(self.n_columns):\\n                    self.this_item_weights[innerItemIndex] /= self.shrink\\n\\n\\n            if self.TopK == 0:\\n\\n                for innerItemIndex in range(self.n_columns):\\n                    self.W_dense[innerItemIndex,itemIndex] = self.this_item_weights[innerItemIndex]\\n\\n            else:\\n\\n                # Sort indices and select TopK\\n                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\\n                #top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\\n\\n                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\\n                # because we avoid sorting elements we already know we don\\'t care about\\n                # - Partition the data to extract the set of TopK items, this set is unsorted\\n                # - Sort only the TopK items, discarding the rest\\n                # - Get the original item index\\n                #\\n\\n                #this_item_weights_np = clone(template_zero, self.this_item_weights_counter, zero=False)\\n                this_item_weights_np = np.zeros(self.n_columns, dtype=np.float64)\\n\\n                # Add weights in the same ordering as the self.this_item_weights_id data structure\\n                for innerItemIndex in range(self.this_item_weights_counter):\\n                    item_id = self.this_item_weights_id[innerItemIndex]\\n                    this_item_weights_np[innerItemIndex] = - self.this_item_weights[item_id]\\n\\n\\n                local_topK = min([self.TopK, self.this_item_weights_counter])\\n\\n                # Get the unordered set of topK items\\n                top_k_partition = np.argpartition(this_item_weights_np, local_topK-1)[0:local_topK]\\n                # Sort only the elements in the partition\\n                top_k_partition_sorting = np.argsort(this_item_weights_np[top_k_partition])\\n                # Get original index\\n                top_k_idx = top_k_partition[top_k_partition_sorting]\\n\\n\\n                # Incrementally build sparse matrix, do not add zeros\\n                for innerItemIndex in range(len(top_k_idx)):\\n\\n                    topKItemIndex = top_k_idx[innerItemIndex]\\n\\n                    item_id = self.this_item_weights_id[topKItemIndex]\\n\\n                    if self.this_item_weights[item_id] != 0.0:\\n\\n                        values[sparse_data_pointer] = self.this_item_weights[item_id]\\n                        rows[sparse_data_pointer] = item_id\\n                        cols[sparse_data_pointer] = itemIndex\\n\\n                        sparse_data_pointer += 1\\n\\n            itemIndex += 1\\n\\n        # End while on columns\\n\\n        if self.TopK == 0:\\n\\n            return np.array(self.W_dense)\\n\\n        else:\\n\\n            values = np.array(values[0:sparse_data_pointer])\\n            rows = np.array(rows[0:sparse_data_pointer])\\n            cols = np.array(cols[0:sparse_data_pointer])\\n\\n            W_sparse = sps.csr_matrix((values, (rows, cols)),\\n                                    shape=(self.n_columns, self.n_columns),\\n                                    dtype=np.float32)\\n\\n            return W_sparse\\n\\n\\ndef cosine_common(X):\\n    \"\"\"\\n    Function that pairwise cosine similarity of the columns in X.\\n    It takes only the values in common between each pair of columns\\n    :param X: instance of scipy.sparse.csc_matrix\\n    :return:\\n        the result of co_prodsum\\n        the number of co_rated elements for every column pair\\n    \"\"\"\\n\\n    X = check_matrix(X, \\'csc\\')\\n\\n    # use Cython MemoryViews for fast access to the sparse structure of X\\n    cdef int [:] indices = X.indices\\n    cdef int [:] indptr = X.indptr\\n    cdef float [:] data = X.data\\n\\n    # initialize the result variables\\n    cdef int n_cols = X.shape[1]\\n    cdef np.ndarray[np.float32_t, ndim=2] result = np.zeros([n_cols, n_cols], dtype=np.float32)\\n    cdef np.ndarray[np.int32_t, ndim=2] common = np.zeros([n_cols, n_cols], dtype=np.int32)\\n\\n    # let\\'s declare all the variables that we\\'ll use in the loop here\\n    # NOTE: declaring the type of your variables makes your Cython code run MUCH faster\\n    # NOTE: Cython allows cdef\\'s only in the main scope\\n    # cdef\\'s in nested codes will result in compilation errors\\n    cdef int current_col, second_col, n_i, n_j, ii, jj, n_common\\n    cdef float ii_sum, jj_sum, ij_sum, x_i, x_j\\n\\n    for current_col in range(n_cols):\\n        n_i = indptr[current_col+1] - indptr[current_col]\\n        # the correlation matrix is symmetric,\\n        # let\\'s compute only the values for the upper-right triangle\\n        for second_col in range(current_col+1, n_cols):\\n            n_j = indptr[second_col+1] - indptr[second_col]\\n\\n            ij_sum, ii_sum, jj_sum = 0.0, 0.0, 0.0\\n            ii, jj = 0, 0\\n            n_common = 0\\n\\n            # here we exploit the fact that the two subvectors in indices are sorted\\n            # to compute the dot product of the rows in common between i and j in linear time.\\n            # (indices[indptr[i]:indptr[i]+n_i] and indices[indptr[j]:indptr[j]+n_j]\\n            # contain the row indices of the non-zero items in columns i and j)\\n            while ii < n_i and jj < n_j:\\n                if indices[indptr[current_col] + ii] < indices[indptr[second_col] + jj]:\\n                    ii += 1\\n                elif indices[indptr[current_col] + ii] > indices[indptr[second_col] + jj]:\\n                    jj += 1\\n                else:\\n                    x_i = data[indptr[current_col] + ii]\\n                    x_j = data[indptr[second_col] + jj]\\n                    ij_sum += x_i * x_j\\n                    ii_sum += x_i ** 2\\n                    jj_sum += x_j ** 2\\n                    ii += 1\\n                    jj += 1\\n                    n_common += 1\\n\\n            if n_common > 0:\\n                result[current_col, second_col] = ij_sum / np.sqrt(ii_sum * jj_sum)\\n                result[second_col, current_col] = result[current_col, second_col]\\n                common[current_col, second_col] = n_common\\n                common[second_col, current_col] = n_common\\n\\n    return result, common\\n\\n\\n\\n###################################################################################################################\\n#########################       ARGSORT\\n\\nfrom libc.stdlib cimport malloc, free#, qsort\\n\\n# Declaring QSORT as \"gil safe\", appending \"nogil\" at the end of the declaration\\n# Otherwise I will not be able to pass the comparator function pointer\\n# https://stackoverflow.com/questions/8353076/how-do-i-pass-a-pointer-to-a-c-function-in-cython\\ncdef extern from \"stdlib.h\":\\n    ctypedef void const_void \"const void\"\\n    void qsort(void *base, int nmemb, int size,\\n            int(*compar)(const_void *, const_void *)) nogil\\n\\n\\n# Node struct\\nctypedef struct matrix_element_s:\\n    long coordinate\\n    double data\\n\\n\\ncdef int compare_struct_on_data(const void * a_input, const void * b_input):\\n    \"\"\"\\n    The function compares the data contained in the two struct passed.\\n    If a.data > b.data returns >0  \\n    If a.data < b.data returns <0      \\n    \\n    :return int: +1 or -1\\n    \"\"\"\\n\\n    cdef matrix_element_s * a_casted = <matrix_element_s *> a_input\\n    cdef matrix_element_s * b_casted = <matrix_element_s *> b_input\\n\\n    if (a_casted.data - b_casted.data) > 0.0:\\n        return +1\\n    else:\\n        return -1\\n\\n\\ncdef long[:] argsort(double[:] this_item_weights, int TopK):\\n\\n    cdef array[long] template_zero = array(\\'l\\')\\n    cdef array[long] result = clone(template_zero, TopK, zero=False)\\n\\n    cdef matrix_element_s *matrix_element_array\\n    cdef int index, num_elements\\n\\n    num_elements = len(this_item_weights)\\n\\n    # Allocate vector that will be used for sorting\\n    matrix_element_array = < matrix_element_s *> malloc(num_elements * sizeof(matrix_element_s))\\n\\n    # Fill vector wit pointers to list elements\\n    for index in range(num_elements):\\n        matrix_element_array[index].coordinate = index\\n        matrix_element_array[index].data = this_item_weights[index]\\n\\n    # Sort array elements on their data field\\n    qsort(matrix_element_array, num_elements, sizeof(matrix_element_s), compare_struct_on_data)\\n\\n    # Sort is from lower to higher, therefore the elements to be considered are from len-topK to len\\n    for index in range(TopK):\\n        result[index] = matrix_element_array[num_elements - index - 1].coordinate\\n\\n    free(matrix_element_array)\\n\\n    return result'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2165\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2166\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2167\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2168\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-128>\u001b[0m in \u001b[0;36mcython\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\Cython\\Build\\IpythonMagic.py\u001b[0m in \u001b[0;36mcython\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         self._build_extension(extension, lib_dir, pgo_step_name='use' if args.pgo else None,\n\u001b[1;32m--> 329\u001b[1;33m                               quiet=args.quiet)\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\Cython\\Build\\IpythonMagic.py\u001b[0m in \u001b[0;36m_build_extension\u001b[1;34m(self, extension, lib_dir, temp_dir, pgo_step_name, quiet)\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mquiet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[0mold_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m             \u001b[0mbuild_extension\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mquiet\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mold_threshold\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;31m# Now actually compile and link everything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_extensions_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mbuild_extensions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    446\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_extensions_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_extensions_serial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_extensions_parallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36m_build_extensions_serial\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextensions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_build_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\distutils\\command\\build_ext.py\u001b[0m in \u001b[0;36mbuild_extension\u001b[1;34m(self, ext)\u001b[0m\n\u001b[0;32m    531\u001b[0m                                          \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m                                          \u001b[0mextra_postargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m                                          depends=ext.depends)\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;31m# XXX outdated variable, kept here in case third-part code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\distutils\\_msvccompiler.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, sources, output_dir, macros, include_dirs, debug, extra_preargs, extra_postargs, depends)\u001b[0m\n\u001b[0;32m    423\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mDistutilsExecError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mCompileError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobjects\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCompileError\u001b[0m: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2017\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.16.27023\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit status 2"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "import time, sys\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone\n",
    "from libc.math cimport sqrt\n",
    "import scipy.sparse as sps\n",
    "\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)\n",
    "\n",
    "    \n",
    "\n",
    "import time, sys\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone\n",
    "from libc.math cimport sqrt\n",
    "import scipy.sparse as sps\n",
    "\n",
    "cdef class Compute_Similarity_Cython:\n",
    "\n",
    "    cdef int TopK\n",
    "    cdef long n_columns, n_rows\n",
    "    cdef double[:] this_item_weights\n",
    "    cdef int[:] this_item_weights_mask, this_item_weights_id\n",
    "    cdef int this_item_weights_counter\n",
    "    cdef int[:] user_to_item_row_ptr, user_to_item_cols\n",
    "    cdef int[:] item_to_user_rows, item_to_user_col_ptr\n",
    "    cdef double[:] user_to_item_data, item_to_user_data\n",
    "    cdef double[:] sumOfSquared, sumOfSquared_to_1_minus_alpha, sumOfSquared_to_alpha\n",
    "    cdef int shrink, normalize, adjusted_cosine, pearson_correlation, tanimoto_coefficient, asymmetric_cosine, dice_coefficient, tversky_coefficient\n",
    "    cdef float asymmetric_alpha, tversky_alpha, tversky_beta\n",
    "    cdef int use_row_weights\n",
    "    cdef double[:] row_weights\n",
    "    cdef double[:,:] W_dense\n",
    "\n",
    "    def __init__(self, dataMatrix, topK = 100, shrink=5, normalize = True, row_weights = None):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity on the columns of dataMatrix\n",
    "        If it is computed on URM=|users|x|items|, pass the URM as is.\n",
    "        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n",
    "        :param dataMatrix:\n",
    "        :param topK:\n",
    "        :param shrink:\n",
    "        :param normalize:           If True divide the dot product by the product of the norms\n",
    "        :param row_weights:         Multiply the values in each row by a specified value. Array\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(Compute_Similarity_Cython, self).__init__()\n",
    "\n",
    "        self.n_columns = dataMatrix.shape[1]\n",
    "        self.n_rows = dataMatrix.shape[0]\n",
    "        self.shrink = shrink\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.TopK = min(topK, self.n_columns)\n",
    "        self.this_item_weights = np.zeros(self.n_columns, dtype=np.float64)\n",
    "        self.this_item_weights_id = np.zeros(self.n_columns, dtype=np.int32)\n",
    "        self.this_item_weights_mask = np.zeros(self.n_columns, dtype=np.int32)\n",
    "        self.this_item_weights_counter = 0\n",
    "\n",
    "        # Copy data to avoid altering the original object\n",
    "        dataMatrix = dataMatrix.copy()\n",
    "\n",
    "        # Compute sum of squared values to be used in normalization\n",
    "        self.sumOfSquared = np.array(dataMatrix.power(2).sum(axis=0), dtype=np.float64).ravel()\n",
    "\n",
    "        # Apply weight after sumOfSquared has been computed but before the matrix is\n",
    "        # split in its inner data structures\n",
    "        self.use_row_weights = False\n",
    "\n",
    "        if row_weights is not None:\n",
    "\n",
    "            if dataMatrix.shape[0] != len(row_weights):\n",
    "                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\n",
    "                                 \"Row_weights has {} rows, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\n",
    "\n",
    "            self.use_row_weights = True\n",
    "            self.row_weights = np.array(row_weights, dtype=np.float64)\n",
    "\n",
    "        dataMatrix = check_matrix(dataMatrix, 'csr')\n",
    "\n",
    "        self.user_to_item_row_ptr = dataMatrix.indptr\n",
    "        self.user_to_item_cols = dataMatrix.indices\n",
    "        self.user_to_item_data = np.array(dataMatrix.data, dtype=np.float64)\n",
    "\n",
    "        dataMatrix = check_matrix(dataMatrix, 'csc')\n",
    "        self.item_to_user_rows = dataMatrix.indices\n",
    "        self.item_to_user_col_ptr = dataMatrix.indptr\n",
    "        self.item_to_user_data = np.array(dataMatrix.data, dtype=np.float64)\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_columns,self.n_columns))\n",
    "\n",
    "\n",
    "    cdef int[:] getUsersThatRatedItem(self, long item_id):\n",
    "        return self.item_to_user_rows[self.item_to_user_col_ptr[item_id]:self.item_to_user_col_ptr[item_id+1]]\n",
    "\n",
    "    cdef int[:] getItemsRatedByUser(self, long user_id):\n",
    "        return self.user_to_item_cols[self.user_to_item_row_ptr[user_id]:self.user_to_item_row_ptr[user_id+1]]\n",
    "\n",
    "\n",
    "    cdef computeItemSimilarities(self, long item_id_input):\n",
    "        \"\"\"\n",
    "    \n",
    "        The implementation here used is:\n",
    "        - Select the first item\n",
    "        - Initialize a zero valued array for the similarities\n",
    "        - Get the users who rated the first item\n",
    "        - Loop through the users\n",
    "        -- Given a user, get the items he rated (second item)\n",
    "        -- Update the similarity of the items he rated\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        cdef long user_index, user_id, item_index, item_id, item_id_second\n",
    "        cdef int[:] users_that_rated_item = self.getUsersThatRatedItem(item_id_input)\n",
    "        cdef int[:] items_rated_by_user\n",
    "        cdef double rating_item_input, rating_item_second, row_weight\n",
    "\n",
    "        # Clean previous item\n",
    "        for item_index in range(self.this_item_weights_counter):\n",
    "            item_id = self.this_item_weights_id[item_index]\n",
    "            self.this_item_weights_mask[item_id] = False\n",
    "            self.this_item_weights[item_id] = 0.0\n",
    "\n",
    "        self.this_item_weights_counter = 0\n",
    "\n",
    "        # Get users that rated the items\n",
    "        for user_index in range(len(users_that_rated_item)):\n",
    "\n",
    "            user_id = users_that_rated_item[user_index]\n",
    "            rating_item_input = self.item_to_user_data[self.item_to_user_col_ptr[item_id_input]+user_index]\n",
    "\n",
    "            if self.use_row_weights:\n",
    "                row_weight = self.row_weights[user_id]\n",
    "            else:\n",
    "                row_weight = 1.0\n",
    "\n",
    "            # Get all items rated by that user\n",
    "            items_rated_by_user = self.getItemsRatedByUser(user_id)\n",
    "\n",
    "            for item_index in range(len(items_rated_by_user)):\n",
    "\n",
    "                item_id_second = items_rated_by_user[item_index]\n",
    "\n",
    "                # Do not compute the similarity on the diagonal\n",
    "                if item_id_second != item_id_input:\n",
    "                    # Increment similairty\n",
    "                    rating_item_second = self.user_to_item_data[self.user_to_item_row_ptr[user_id]+item_index]\n",
    "\n",
    "                    self.this_item_weights[item_id_second] += rating_item_input*rating_item_second*row_weight\n",
    "\n",
    "                    # Update global data structure\n",
    "                    if not self.this_item_weights_mask[item_id_second]:\n",
    "\n",
    "                        self.this_item_weights_mask[item_id_second] = True\n",
    "                        self.this_item_weights_id[self.this_item_weights_counter] = item_id_second\n",
    "                        self.this_item_weights_counter += 1\n",
    "\n",
    "\n",
    "    def compute_similarity(self, start_col=None, end_col=None):\n",
    "        \"\"\"\n",
    "        Compute the similarity for the given dataset\n",
    "        :param self:\n",
    "        :param start_col: column to begin with\n",
    "        :param end_col: column to stop before, end_col is excluded\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        cdef int print_block_size = 500\n",
    "        cdef int itemIndex, innerItemIndex, item_id, local_topK\n",
    "        cdef long long topKItemIndex\n",
    "        cdef long long[:] top_k_idx\n",
    "\n",
    "        # Declare numpy data type to use vetor indexing and simplify the topK selection code\n",
    "        cdef np.ndarray[long, ndim=1] top_k_partition, top_k_partition_sorting\n",
    "        cdef np.ndarray[np.float64_t, ndim=1] this_item_weights_np\n",
    "        cdef long processedItems = 0\n",
    "\n",
    "        # Data structure to incrementally build sparse matrix\n",
    "        # Preinitialize max possible length\n",
    "        cdef double[:] values = np.zeros((self.n_columns*self.TopK))\n",
    "        cdef int[:] rows = np.zeros((self.n_columns*self.TopK,), dtype=np.int32)\n",
    "        cdef int[:] cols = np.zeros((self.n_columns*self.TopK,), dtype=np.int32)\n",
    "        cdef long sparse_data_pointer = 0\n",
    "        cdef int start_col_local = 0, end_col_local = self.n_columns\n",
    "        cdef array[double] template_zero = array('d')\n",
    "\n",
    "        if start_col is not None and start_col>0 and start_col<self.n_columns:\n",
    "            start_col_local = start_col\n",
    "\n",
    "        if end_col is not None and end_col>start_col_local and end_col<self.n_columns:\n",
    "            end_col_local = end_col\n",
    "\n",
    "        start_time = time.time()\n",
    "        last_print_time = start_time\n",
    "\n",
    "        itemIndex = start_col_local\n",
    "\n",
    "        # Compute all similarities for each item\n",
    "        while itemIndex < end_col_local:\n",
    "\n",
    "            processedItems += 1\n",
    "\n",
    "            if processedItems % print_block_size==0 or processedItems==end_col_local:\n",
    "\n",
    "                current_time = time.time()\n",
    "\n",
    "                # Set block size to the number of items necessary in order to print every 30 seconds\n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "                print_block_size = int(itemPerSec*30)\n",
    "\n",
    "                if current_time - last_print_time > 30  or processedItems==end_col_local:\n",
    "\n",
    "                    print(\"Similarity column {} ( {:2.0f} % ), {:.2f} column/sec, elapsed time {:.2f} min\".format(\n",
    "                        processedItems, processedItems*1.0/(end_col_local-start_col_local)*100, itemPerSec, (time.time()-start_time) / 60))\n",
    "\n",
    "                    last_print_time = current_time\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "                    sys.stderr.flush()\n",
    "\n",
    "\n",
    "            # Computed similarities go in self.this_item_weights\n",
    "            self.computeItemSimilarities(itemIndex)\n",
    "\n",
    "            # Apply normalization and shrinkage, ensure denominator != 0\n",
    "            if self.normalize:\n",
    "                for innerItemIndex in range(self.n_columns):\n",
    "\n",
    "                    if self.asymmetric_cosine:\n",
    "                        self.this_item_weights[innerItemIndex] /= self.sumOfSquared_to_alpha[itemIndex] * self.sumOfSquared_to_1_minus_alpha[innerItemIndex]\\\n",
    "                                                             + self.shrink + 1e-6\n",
    "\n",
    "                    else:\n",
    "                        self.this_item_weights[innerItemIndex] /= self.sumOfSquared[itemIndex] * self.sumOfSquared[innerItemIndex]\\\n",
    "                                                             + self.shrink + 1e-6\n",
    "\n",
    "            elif self.shrink != 0:\n",
    "                for innerItemIndex in range(self.n_columns):\n",
    "                    self.this_item_weights[innerItemIndex] /= self.shrink\n",
    "\n",
    "\n",
    "            if self.TopK == 0:\n",
    "\n",
    "                for innerItemIndex in range(self.n_columns):\n",
    "                    self.W_dense[innerItemIndex,itemIndex] = self.this_item_weights[innerItemIndex]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Sort indices and select TopK\n",
    "                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\n",
    "                #top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\n",
    "\n",
    "                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                # because we avoid sorting elements we already know we don't care about\n",
    "                # - Partition the data to extract the set of TopK items, this set is unsorted\n",
    "                # - Sort only the TopK items, discarding the rest\n",
    "                # - Get the original item index\n",
    "                #\n",
    "\n",
    "                #this_item_weights_np = clone(template_zero, self.this_item_weights_counter, zero=False)\n",
    "                this_item_weights_np = np.zeros(self.n_columns, dtype=np.float64)\n",
    "\n",
    "                # Add weights in the same ordering as the self.this_item_weights_id data structure\n",
    "                for innerItemIndex in range(self.this_item_weights_counter):\n",
    "                    item_id = self.this_item_weights_id[innerItemIndex]\n",
    "                    this_item_weights_np[innerItemIndex] = - self.this_item_weights[item_id]\n",
    "\n",
    "\n",
    "                local_topK = min([self.TopK, self.this_item_weights_counter])\n",
    "\n",
    "                # Get the unordered set of topK items\n",
    "                top_k_partition = np.argpartition(this_item_weights_np, local_topK-1)[0:local_topK]\n",
    "                # Sort only the elements in the partition\n",
    "                top_k_partition_sorting = np.argsort(this_item_weights_np[top_k_partition])\n",
    "                # Get original index\n",
    "                top_k_idx = top_k_partition[top_k_partition_sorting]\n",
    "\n",
    "\n",
    "                # Incrementally build sparse matrix, do not add zeros\n",
    "                for innerItemIndex in range(len(top_k_idx)):\n",
    "\n",
    "                    topKItemIndex = top_k_idx[innerItemIndex]\n",
    "\n",
    "                    item_id = self.this_item_weights_id[topKItemIndex]\n",
    "\n",
    "                    if self.this_item_weights[item_id] != 0.0:\n",
    "\n",
    "                        values[sparse_data_pointer] = self.this_item_weights[item_id]\n",
    "                        rows[sparse_data_pointer] = item_id\n",
    "                        cols[sparse_data_pointer] = itemIndex\n",
    "\n",
    "                        sparse_data_pointer += 1\n",
    "\n",
    "            itemIndex += 1\n",
    "\n",
    "        # End while on columns\n",
    "\n",
    "        if self.TopK == 0:\n",
    "\n",
    "            return np.array(self.W_dense)\n",
    "\n",
    "        else:\n",
    "\n",
    "            values = np.array(values[0:sparse_data_pointer])\n",
    "            rows = np.array(rows[0:sparse_data_pointer])\n",
    "            cols = np.array(cols[0:sparse_data_pointer])\n",
    "\n",
    "            W_sparse = sps.csr_matrix((values, (rows, cols)),\n",
    "                                    shape=(self.n_columns, self.n_columns),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "\n",
    "def cosine_common(X):\n",
    "    \"\"\"\n",
    "    Function that pairwise cosine similarity of the columns in X.\n",
    "    It takes only the values in common between each pair of columns\n",
    "    :param X: instance of scipy.sparse.csc_matrix\n",
    "    :return:\n",
    "        the result of co_prodsum\n",
    "        the number of co_rated elements for every column pair\n",
    "    \"\"\"\n",
    "\n",
    "    X = check_matrix(X, 'csc')\n",
    "\n",
    "    # use Cython MemoryViews for fast access to the sparse structure of X\n",
    "    cdef int [:] indices = X.indices\n",
    "    cdef int [:] indptr = X.indptr\n",
    "    cdef float [:] data = X.data\n",
    "\n",
    "    # initialize the result variables\n",
    "    cdef int n_cols = X.shape[1]\n",
    "    cdef np.ndarray[np.float32_t, ndim=2] result = np.zeros([n_cols, n_cols], dtype=np.float32)\n",
    "    cdef np.ndarray[np.int32_t, ndim=2] common = np.zeros([n_cols, n_cols], dtype=np.int32)\n",
    "\n",
    "    # let's declare all the variables that we'll use in the loop here\n",
    "    # NOTE: declaring the type of your variables makes your Cython code run MUCH faster\n",
    "    # NOTE: Cython allows cdef's only in the main scope\n",
    "    # cdef's in nested codes will result in compilation errors\n",
    "    cdef int current_col, second_col, n_i, n_j, ii, jj, n_common\n",
    "    cdef float ii_sum, jj_sum, ij_sum, x_i, x_j\n",
    "\n",
    "    for current_col in range(n_cols):\n",
    "        n_i = indptr[current_col+1] - indptr[current_col]\n",
    "        # the correlation matrix is symmetric,\n",
    "        # let's compute only the values for the upper-right triangle\n",
    "        for second_col in range(current_col+1, n_cols):\n",
    "            n_j = indptr[second_col+1] - indptr[second_col]\n",
    "\n",
    "            ij_sum, ii_sum, jj_sum = 0.0, 0.0, 0.0\n",
    "            ii, jj = 0, 0\n",
    "            n_common = 0\n",
    "\n",
    "            # here we exploit the fact that the two subvectors in indices are sorted\n",
    "            # to compute the dot product of the rows in common between i and j in linear time.\n",
    "            # (indices[indptr[i]:indptr[i]+n_i] and indices[indptr[j]:indptr[j]+n_j]\n",
    "            # contain the row indices of the non-zero items in columns i and j)\n",
    "            while ii < n_i and jj < n_j:\n",
    "                if indices[indptr[current_col] + ii] < indices[indptr[second_col] + jj]:\n",
    "                    ii += 1\n",
    "                elif indices[indptr[current_col] + ii] > indices[indptr[second_col] + jj]:\n",
    "                    jj += 1\n",
    "                else:\n",
    "                    x_i = data[indptr[current_col] + ii]\n",
    "                    x_j = data[indptr[second_col] + jj]\n",
    "                    ij_sum += x_i * x_j\n",
    "                    ii_sum += x_i ** 2\n",
    "                    jj_sum += x_j ** 2\n",
    "                    ii += 1\n",
    "                    jj += 1\n",
    "                    n_common += 1\n",
    "\n",
    "            if n_common > 0:\n",
    "                result[current_col, second_col] = ij_sum / np.sqrt(ii_sum * jj_sum)\n",
    "                result[second_col, current_col] = result[current_col, second_col]\n",
    "                common[current_col, second_col] = n_common\n",
    "                common[second_col, current_col] = n_common\n",
    "\n",
    "    return result, common\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "#########################       ARGSORT\n",
    "\n",
    "from libc.stdlib cimport malloc, free#, qsort\n",
    "\n",
    "# Declaring QSORT as \"gil safe\", appending \"nogil\" at the end of the declaration\n",
    "# Otherwise I will not be able to pass the comparator function pointer\n",
    "# https://stackoverflow.com/questions/8353076/how-do-i-pass-a-pointer-to-a-c-function-in-cython\n",
    "cdef extern from \"stdlib.h\":\n",
    "    ctypedef void const_void \"const void\"\n",
    "    void qsort(void *base, int nmemb, int size,\n",
    "            int(*compar)(const_void *, const_void *)) nogil\n",
    "\n",
    "\n",
    "# Node struct\n",
    "ctypedef struct matrix_element_s:\n",
    "    long coordinate\n",
    "    double data\n",
    "\n",
    "\n",
    "cdef int compare_struct_on_data(const void * a_input, const void * b_input):\n",
    "    \"\"\"\n",
    "    The function compares the data contained in the two struct passed.\n",
    "    If a.data > b.data returns >0  \n",
    "    If a.data < b.data returns <0      \n",
    "    \n",
    "    :return int: +1 or -1\n",
    "    \"\"\"\n",
    "\n",
    "    cdef matrix_element_s * a_casted = <matrix_element_s *> a_input\n",
    "    cdef matrix_element_s * b_casted = <matrix_element_s *> b_input\n",
    "\n",
    "    if (a_casted.data - b_casted.data) > 0.0:\n",
    "        return +1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "cdef long[:] argsort(double[:] this_item_weights, int TopK):\n",
    "\n",
    "    cdef array[long] template_zero = array('l')\n",
    "    cdef array[long] result = clone(template_zero, TopK, zero=False)\n",
    "\n",
    "    cdef matrix_element_s *matrix_element_array\n",
    "    cdef int index, num_elements\n",
    "\n",
    "    num_elements = len(this_item_weights)\n",
    "\n",
    "    # Allocate vector that will be used for sorting\n",
    "    matrix_element_array = < matrix_element_s *> malloc(num_elements * sizeof(matrix_element_s))\n",
    "\n",
    "    # Fill vector wit pointers to list elements\n",
    "    for index in range(num_elements):\n",
    "        matrix_element_array[index].coordinate = index\n",
    "        matrix_element_array[index].data = this_item_weights[index]\n",
    "\n",
    "    # Sort array elements on their data field\n",
    "    qsort(matrix_element_array, num_elements, sizeof(matrix_element_s), compare_struct_on_data)\n",
    "\n",
    "    # Sort is from lower to higher, therefore the elements to be considered are from len-topK to len\n",
    "    for index in range(TopK):\n",
    "        result[index] = matrix_element_array[num_elements - index - 1].coordinate\n",
    "\n",
    "    free(matrix_element_array)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "\n",
    "def precision(is_relevant, relevant_items):\n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    return precision_score\n",
    "\n",
    "def recall(is_relevant, relevant_items):\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    return recall_score\n",
    "\n",
    "def MAP(is_relevant, relevant_items):\n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "    return map_score\n",
    "\n",
    "def evaluate_algorithm(URM_test, target_playlists, recommender_object, alfa, beta, at=10):\n",
    "\n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "\n",
    "    num_eval = 0\n",
    "\n",
    "    URM_test = sc.sparse.csr_matrix(URM_test)\n",
    "\n",
    "    n_users = URM_test.shape[0]\n",
    "\n",
    "    for user_id in target_playlists:\n",
    "        \n",
    "        start_pos = URM_test.indptr[user_id]\n",
    "        end_pos = URM_test.indptr[user_id+1]\n",
    "\n",
    "        if end_pos-start_pos>0:\n",
    "\n",
    "            relevant_items = URM_test.indices[start_pos:end_pos]\n",
    "            \n",
    "            recommended_items = recommender_object.recommend(user_id, alfa, beta, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "            cumulative_precision += precision(is_relevant, relevant_items)\n",
    "            cumulative_recall += recall(is_relevant, relevant_items)\n",
    "            cumulative_MAP += MAP(is_relevant, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "\n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "\n",
    "    result_dict = {\n",
    "        \"precision\": cumulative_precision,\n",
    "        \"recall\": cumulative_recall,\n",
    "        \"MAP\": cumulative_MAP,\n",
    "    }\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter\n",
    "def train_test_holdout(URM_all, train_perc = 0.8):\n",
    "\n",
    "    numInteractions = URM_all.nnz\n",
    "    URM_all = URM_all.tocoo()\n",
    "\n",
    "    train_mask = np.random.choice([True,False], numInteractions, [train_perc, 1-train_perc])\n",
    "\n",
    "    URM_train = sps.coo_matrix((URM_all.data[train_mask], (URM_all.row[train_mask], URM_all.col[train_mask])))\n",
    "    URM_train = URM_train.tocsr()\n",
    "\n",
    "    test_mask = np.logical_not(train_mask)\n",
    "\n",
    "    URM_test = sps.coo_matrix((URM_all.data[test_mask], (URM_all.row[test_mask], URM_all.col[test_mask])))\n",
    "    URM_test = URM_test.tocsr()\n",
    "\n",
    "    return URM_train, URM_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_recommender(object):\n",
    "    \n",
    "    def __init__(self, URM, ICM):\n",
    "        self.URM = URM\n",
    "        self.ICM = ICM\n",
    "            \n",
    "    def fit_content_based(self, topK=50, shrink=100, normalize = True):\n",
    "        \n",
    "        similarity_object_content_based = Compute_Similarity_Cython(self.ICM.T, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize)\n",
    "    \n",
    "        self.W_sparse_content_based = similarity_object_content_based.compute_similarity()\n",
    "    \n",
    "    def fit_item_based(self, topK=50, shrink=100, normalize = True):\n",
    "        \n",
    "        similarity_object_item_cf = Compute_Similarity_Cython(self.URM, shrink=shrink, \n",
    "                                                  topK=topK, normalize=normalize)\n",
    "    \n",
    "        self.W_sparse_item_cf = similarity_object_item_cf.compute_similarity()\n",
    "    \n",
    "    def fit_user_based(self, topK=50, shrink=100, normalize = True):\n",
    "        \n",
    "        similarity_object_user_based = Compute_Similarity_Cython(self.URM.T, shrink=shrink, \n",
    "                                                                topK=topK, normalize=normalize)\n",
    "        \n",
    "        self.W_sparse_user_cf = similarity_object_user_based.compute_similarity()\n",
    "    \n",
    "    def recommend(self, user_id, alfa, beta, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        user_profile_ub = self.W_sparse_user_cf[user_id]\n",
    "        \n",
    "        scores_content_based = user_profile.dot(self.W_sparse_content_based).toarray().ravel()\n",
    "        scores_item_cf = user_profile.dot(self.W_sparse_item_cf).toarray().ravel()\n",
    "        scores_user_cf = user_profile_ub.dot(self.URM).toarray().ravel()\n",
    "        \n",
    "        scores = (1 - alfa - beta) * scores_content_based + alfa * scores_item_cf + beta * scores_user_cf\n",
    "        \n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "\n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "\n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_train, URM_test = train_test_holdout(URM_csr, train_perc = 0.8)\n",
    "\n",
    "# to avoid dimension mismatch due to the random nature of the split\n",
    "t1 = np.shape(URM_train)\n",
    "t2 = np.shape(URM_test)\n",
    "while(t1 != t2):\n",
    "    URM_train, URM_test = train_test_holdout(URM_csr, train_perc = 0.8)\n",
    "    t1 = np.shape(URM_train)\n",
    "    t2 = np.shape(URM_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target_playlists[\"playlist_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning the hybrid recommender\n",
    "recommender = Hybrid_recommender(URM_train, ICM_csr)\n",
    "\n",
    "# parameters to be tuned: cb_shrink, ib_shrink, ub_shrink, cb_topk, ib_topk, ub_topk, alfa, beta\n",
    "#alfa = 0.5\n",
    "#beta = 0.3\n",
    "#cb_topk = 750\n",
    "#ib_topk = 750\n",
    "#ub_topk = 100\n",
    "#cb_shrink = 3\n",
    "#ib_shrink = 20\n",
    "#ub_shrink = 20\n",
    "\n",
    "\n",
    "cb_topk_range = [750, 1000]\n",
    "ib_topk_range = [750, 1000]\n",
    "ub_topk_range = [50, 100, 500]\n",
    "\n",
    "cb_shrink_range = [5]\n",
    "ib_shrink_range = [20]\n",
    "ub_shrink_range = [20, 50]\n",
    "\n",
    "alfa_range = [0.5, 0.6]\n",
    "beta_range = [0.3, 0.4]\n",
    "\n",
    "x = []\n",
    "x_verbose = []\n",
    "map_res = []\n",
    "prec_res = []\n",
    "rec_res = []\n",
    "index = 1\n",
    "\n",
    "\n",
    "\n",
    "for alfa in alfa_range:\n",
    "    for beta in beta_range:\n",
    "        for cb_shrink in cb_shrink_range:\n",
    "            for ib_shrink in ib_shrink_range:\n",
    "                for ub_shrink in ub_shrink_range:\n",
    "                    for cb_topk in cb_topk_range:\n",
    "                        for ib_topk in ib_topk_range:\n",
    "                            for ub_topk in ub_topk_range:\n",
    "                                recommender.fit_content_based(shrink=cb_shrink, topK=cb_topk)\n",
    "                                recommender.fit_item_based(shrink=ib_shrink, topK=ib_topk)\n",
    "                                recommender.fit_user_based(shrink=ub_shrink, topK=ub_topk)\n",
    "\n",
    "                                temp_result = evaluate_algorithm(URM_test, target, recommender, alfa, beta, 10)\n",
    "                                map_res.append(temp_result[\"MAP\"])\n",
    "                                prec_res.append(temp_result[\"precision\"])\n",
    "                                rec_res.append(temp_result[\"recall\"])\n",
    "\n",
    "                                x.append(index)\n",
    "                                x_verbose.append(\"ITERATION \" + str(index) + '\\n' +\n",
    "                                                 \"alfa =  \" + str(alfa) + '\\n' + \n",
    "                                                 \"beta = \" + str(beta) + '\\n' + \n",
    "                                                 \"topk(cb,ib,ub) = \" + str(cb_topk) + ',' + str(ib_topk) + ',' + str(ub_topk) + '\\n' + \n",
    "                                                 \"shrink(cb,ib,ub) = \" + str(cb_shrink) + ',' + str(ib_shrink) + ',' + str(ub_shrink) + '\\n' +\n",
    "                                                 \"MAP = \" + str(temp_result[\"MAP\"]) + '\\n') \n",
    "\n",
    "                                index += 1\n",
    "\n",
    "\n",
    "\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(x, map_res)\n",
    "pyplot.ylabel('MAP')\n",
    "pyplot.xlabel('iteration')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for triple in x_verbose:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_output_file():\n",
    "    file = open(\"submission.csv\", 'a')\n",
    "    file.write(\"playlist_id,track_ids\" + '\\n')\n",
    "    return file\n",
    "\n",
    "# useful to print to file with the right structure\n",
    "def print_to_file(playlist, tracks, file):\n",
    "    file.write(str(playlist) + ',')\n",
    "    index = 0\n",
    "    while index < 9:\n",
    "        file.write(str(tracks[index]) + ' ')\n",
    "        index += 1\n",
    "    file.write(str(tracks[index]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution of the recommendations for submission\n",
    "file = initialize_output_file()\n",
    "\n",
    "recommender = Hybrid_recommender(URM_csr, ICM_csr)\n",
    "\n",
    "recommender.fit_content_based(shrink=5, topK=100)\n",
    "recommender.fit_item_based(shrink=20, topK=200)\n",
    "recommender.fit_user_based(shrink=20, topK=100)\n",
    "\n",
    "alfa = 0.43\n",
    "beta = 0.37\n",
    "\n",
    "for playlist in target_playlists.itertuples(index=True, name='Pandas'):\n",
    "    playlist_id = getattr(playlist, \"playlist_id\")\n",
    "    tracks = recommender.recommend(playlist_id, alfa, beta, 10, True)\n",
    "    print_to_file(playlist_id, tracks, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
